{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Training..\n",
      "Epoch: 1\n",
      "Train Loss: 0.520 | Acc: 24.608%\n",
      "Test Loss: 0.057 | Acc: 37.896%\n",
      "\n",
      "Epoch: 2\n",
      "Train Loss: 0.270 | Acc: 35.745%\n",
      "Test Loss: 0.053 | Acc: 48.080%\n",
      "\n",
      "Epoch: 3\n",
      "Train Loss: 0.254 | Acc: 47.082%\n",
      "Test Loss: 0.050 | Acc: 58.097%\n",
      "\n",
      "Epoch: 4\n",
      "Train Loss: 0.239 | Acc: 62.888%\n",
      "Test Loss: 0.047 | Acc: 66.778%\n",
      "\n",
      "Epoch: 5\n",
      "Train Loss: 0.226 | Acc: 67.656%\n",
      "Test Loss: 0.044 | Acc: 68.781%\n",
      "\n",
      "Epoch: 6\n",
      "Train Loss: 0.215 | Acc: 68.856%\n",
      "Test Loss: 0.042 | Acc: 71.786%\n",
      "\n",
      "Epoch: 7\n",
      "Train Loss: 0.204 | Acc: 69.990%\n",
      "Test Loss: 0.040 | Acc: 71.452%\n",
      "\n",
      "Epoch: 8\n",
      "Train Loss: 0.192 | Acc: 71.057%\n",
      "Test Loss: 0.037 | Acc: 69.950%\n",
      "\n",
      "Epoch: 9\n",
      "Train Loss: 0.183 | Acc: 71.190%\n",
      "Test Loss: 0.035 | Acc: 72.120%\n",
      "\n",
      "Epoch: 10\n",
      "Train Loss: 0.173 | Acc: 72.024%\n",
      "Test Loss: 0.033 | Acc: 72.454%\n",
      "\n",
      "Epoch: 11\n",
      "Train Loss: 0.164 | Acc: 73.158%\n",
      "Test Loss: 0.032 | Acc: 74.791%\n",
      "\n",
      "Epoch: 12\n",
      "Train Loss: 0.155 | Acc: 74.091%\n",
      "Test Loss: 0.030 | Acc: 75.793%\n",
      "\n",
      "Epoch: 13\n",
      "Train Loss: 0.148 | Acc: 74.658%\n",
      "Test Loss: 0.029 | Acc: 73.957%\n",
      "\n",
      "Epoch: 14\n",
      "Train Loss: 0.142 | Acc: 74.625%\n",
      "Test Loss: 0.027 | Acc: 75.125%\n",
      "\n",
      "Epoch: 15\n",
      "Train Loss: 0.135 | Acc: 76.125%\n",
      "Test Loss: 0.026 | Acc: 76.628%\n",
      "\n",
      "Epoch: 16\n",
      "Train Loss: 0.130 | Acc: 76.292%\n",
      "Test Loss: 0.025 | Acc: 76.461%\n",
      "\n",
      "Epoch: 17\n",
      "Train Loss: 0.123 | Acc: 78.226%\n",
      "Test Loss: 0.024 | Acc: 78.631%\n",
      "\n",
      "Epoch: 18\n",
      "Train Loss: 0.118 | Acc: 78.193%\n",
      "Test Loss: 0.023 | Acc: 80.634%\n",
      "\n",
      "Epoch: 19\n",
      "Train Loss: 0.112 | Acc: 80.293%\n",
      "Test Loss: 0.021 | Acc: 80.134%\n",
      "\n",
      "Epoch: 20\n",
      "Train Loss: 0.108 | Acc: 80.093%\n",
      "Test Loss: 0.021 | Acc: 80.134%\n",
      "\n",
      "Epoch: 21\n",
      "Train Loss: 0.103 | Acc: 82.261%\n",
      "Test Loss: 0.020 | Acc: 83.139%\n",
      "\n",
      "Epoch: 22\n",
      "Train Loss: 0.100 | Acc: 82.628%\n",
      "Test Loss: 0.019 | Acc: 80.134%\n",
      "\n",
      "Epoch: 23\n",
      "Train Loss: 0.095 | Acc: 83.928%\n",
      "Test Loss: 0.019 | Acc: 86.644%\n",
      "\n",
      "Epoch: 24\n",
      "Train Loss: 0.092 | Acc: 85.762%\n",
      "Test Loss: 0.017 | Acc: 85.977%\n",
      "\n",
      "Epoch: 25\n",
      "Train Loss: 0.088 | Acc: 86.762%\n",
      "Test Loss: 0.017 | Acc: 87.813%\n",
      "\n",
      "Epoch: 26\n",
      "Train Loss: 0.085 | Acc: 86.762%\n",
      "Test Loss: 0.016 | Acc: 88.982%\n",
      "\n",
      "Epoch: 27\n",
      "Train Loss: 0.082 | Acc: 88.029%\n",
      "Test Loss: 0.016 | Acc: 87.646%\n",
      "\n",
      "Epoch: 28\n",
      "Train Loss: 0.079 | Acc: 87.996%\n",
      "Test Loss: 0.015 | Acc: 89.816%\n",
      "\n",
      "Epoch: 29\n",
      "Train Loss: 0.078 | Acc: 87.996%\n",
      "Test Loss: 0.015 | Acc: 88.314%\n",
      "\n",
      "Epoch: 30\n",
      "Train Loss: 0.073 | Acc: 89.463%\n",
      "Test Loss: 0.014 | Acc: 90.818%\n",
      "\n",
      "Epoch: 31\n",
      "Train Loss: 0.072 | Acc: 89.597%\n",
      "Test Loss: 0.014 | Acc: 90.985%\n",
      "\n",
      "Epoch: 32\n",
      "Train Loss: 0.069 | Acc: 89.630%\n",
      "Test Loss: 0.013 | Acc: 90.150%\n",
      "\n",
      "Epoch: 33\n",
      "Train Loss: 0.067 | Acc: 90.130%\n",
      "Test Loss: 0.013 | Acc: 91.486%\n",
      "\n",
      "Epoch: 34\n",
      "Train Loss: 0.065 | Acc: 90.564%\n",
      "Test Loss: 0.012 | Acc: 91.987%\n",
      "\n",
      "Epoch: 35\n",
      "Train Loss: 0.063 | Acc: 90.697%\n",
      "Test Loss: 0.012 | Acc: 91.653%\n",
      "\n",
      "Epoch: 36\n",
      "Train Loss: 0.061 | Acc: 91.164%\n",
      "Test Loss: 0.012 | Acc: 91.987%\n",
      "\n",
      "Epoch: 37\n",
      "Train Loss: 0.060 | Acc: 91.130%\n",
      "Test Loss: 0.011 | Acc: 91.486%\n",
      "\n",
      "Epoch: 38\n",
      "Train Loss: 0.058 | Acc: 90.964%\n",
      "Test Loss: 0.011 | Acc: 91.987%\n",
      "\n",
      "Epoch: 39\n",
      "Train Loss: 0.056 | Acc: 91.664%\n",
      "Test Loss: 0.011 | Acc: 91.152%\n",
      "\n",
      "Epoch: 40\n",
      "Train Loss: 0.056 | Acc: 91.030%\n",
      "Test Loss: 0.011 | Acc: 91.987%\n",
      "\n",
      "Epoch: 41\n",
      "Train Loss: 0.053 | Acc: 91.964%\n",
      "Test Loss: 0.010 | Acc: 92.154%\n",
      "\n",
      "Epoch: 42\n",
      "Train Loss: 0.052 | Acc: 91.731%\n",
      "Test Loss: 0.010 | Acc: 92.487%\n",
      "\n",
      "Epoch: 43\n",
      "Train Loss: 0.051 | Acc: 91.931%\n",
      "Test Loss: 0.010 | Acc: 92.487%\n",
      "\n",
      "Epoch: 44\n",
      "Train Loss: 0.051 | Acc: 92.397%\n",
      "Test Loss: 0.010 | Acc: 92.487%\n",
      "\n",
      "Epoch: 45\n",
      "Train Loss: 0.050 | Acc: 92.264%\n",
      "Test Loss: 0.010 | Acc: 91.319%\n",
      "\n",
      "Epoch: 46\n",
      "Train Loss: 0.049 | Acc: 92.197%\n",
      "Test Loss: 0.010 | Acc: 91.820%\n",
      "\n",
      "Epoch: 47\n",
      "Train Loss: 0.048 | Acc: 92.531%\n",
      "Test Loss: 0.009 | Acc: 92.154%\n",
      "\n",
      "Epoch: 48\n",
      "Train Loss: 0.047 | Acc: 92.331%\n",
      "Test Loss: 0.009 | Acc: 92.154%\n",
      "\n",
      "Epoch: 49\n",
      "Train Loss: 0.045 | Acc: 92.364%\n",
      "Test Loss: 0.009 | Acc: 92.487%\n",
      "\n",
      "Epoch: 50\n",
      "Train Loss: 0.044 | Acc: 92.764%\n",
      "Test Loss: 0.009 | Acc: 92.321%\n",
      "\n",
      "Epoch: 51\n",
      "Train Loss: 0.043 | Acc: 92.731%\n",
      "Test Loss: 0.009 | Acc: 92.487%\n",
      "\n",
      "Epoch: 52\n",
      "Train Loss: 0.043 | Acc: 92.798%\n",
      "Test Loss: 0.008 | Acc: 92.821%\n",
      "\n",
      "Epoch: 53\n",
      "Train Loss: 0.043 | Acc: 92.497%\n",
      "Test Loss: 0.008 | Acc: 92.654%\n",
      "\n",
      "Epoch: 54\n",
      "Train Loss: 0.041 | Acc: 92.798%\n",
      "Test Loss: 0.008 | Acc: 92.654%\n",
      "\n",
      "Epoch: 55\n",
      "Train Loss: 0.040 | Acc: 93.064%\n",
      "Test Loss: 0.008 | Acc: 92.654%\n",
      "\n",
      "Epoch: 56\n",
      "Train Loss: 0.040 | Acc: 92.998%\n",
      "Test Loss: 0.008 | Acc: 92.988%\n",
      "\n",
      "Epoch: 57\n",
      "Train Loss: 0.039 | Acc: 93.031%\n",
      "Test Loss: 0.008 | Acc: 92.821%\n",
      "\n",
      "Epoch: 58\n",
      "Train Loss: 0.039 | Acc: 93.264%\n",
      "Test Loss: 0.008 | Acc: 92.654%\n",
      "\n",
      "Epoch: 59\n",
      "Train Loss: 0.039 | Acc: 93.031%\n",
      "Test Loss: 0.007 | Acc: 93.823%\n",
      "\n",
      "Epoch: 60\n",
      "Train Loss: 0.037 | Acc: 93.231%\n",
      "Test Loss: 0.008 | Acc: 92.321%\n",
      "\n",
      "Epoch: 61\n",
      "Train Loss: 0.037 | Acc: 93.098%\n",
      "Test Loss: 0.007 | Acc: 93.155%\n",
      "\n",
      "Epoch: 62\n",
      "Train Loss: 0.036 | Acc: 93.264%\n",
      "Test Loss: 0.007 | Acc: 92.988%\n",
      "\n",
      "Epoch: 63\n",
      "Train Loss: 0.036 | Acc: 93.331%\n",
      "Test Loss: 0.007 | Acc: 93.489%\n",
      "\n",
      "Epoch: 64\n",
      "Train Loss: 0.036 | Acc: 93.331%\n",
      "Test Loss: 0.008 | Acc: 92.654%\n",
      "\n",
      "Epoch: 65\n",
      "Train Loss: 0.036 | Acc: 93.464%\n",
      "Test Loss: 0.007 | Acc: 93.322%\n",
      "\n",
      "Epoch: 66\n",
      "Train Loss: 0.034 | Acc: 93.364%\n",
      "Test Loss: 0.007 | Acc: 93.322%\n",
      "\n",
      "Epoch: 67\n",
      "Train Loss: 0.034 | Acc: 93.331%\n",
      "Test Loss: 0.007 | Acc: 93.489%\n",
      "\n",
      "Epoch: 68\n",
      "Train Loss: 0.033 | Acc: 93.431%\n",
      "Test Loss: 0.007 | Acc: 92.988%\n",
      "\n",
      "Epoch: 69\n",
      "Train Loss: 0.034 | Acc: 93.364%\n",
      "Test Loss: 0.006 | Acc: 93.656%\n",
      "\n",
      "Epoch: 70\n",
      "Train Loss: 0.033 | Acc: 93.231%\n",
      "Test Loss: 0.006 | Acc: 93.489%\n",
      "\n",
      "Epoch: 71\n",
      "Train Loss: 0.033 | Acc: 93.364%\n",
      "Test Loss: 0.007 | Acc: 92.654%\n",
      "\n",
      "Epoch: 72\n",
      "Train Loss: 0.032 | Acc: 93.498%\n",
      "Test Loss: 0.006 | Acc: 93.489%\n",
      "\n",
      "Epoch: 73\n",
      "Train Loss: 0.031 | Acc: 93.398%\n",
      "Test Loss: 0.006 | Acc: 93.489%\n",
      "\n",
      "Epoch: 74\n",
      "Train Loss: 0.031 | Acc: 93.565%\n",
      "Test Loss: 0.007 | Acc: 92.821%\n",
      "\n",
      "Epoch: 75\n",
      "Train Loss: 0.030 | Acc: 93.598%\n",
      "Test Loss: 0.006 | Acc: 93.823%\n",
      "\n",
      "Epoch: 76\n",
      "Train Loss: 0.031 | Acc: 93.431%\n",
      "Test Loss: 0.006 | Acc: 93.322%\n",
      "\n",
      "Epoch: 77\n",
      "Train Loss: 0.031 | Acc: 93.531%\n",
      "Test Loss: 0.006 | Acc: 92.988%\n",
      "\n",
      "Epoch: 78\n",
      "Train Loss: 0.029 | Acc: 93.498%\n",
      "Test Loss: 0.006 | Acc: 93.489%\n",
      "\n",
      "Epoch: 79\n",
      "Train Loss: 0.029 | Acc: 93.531%\n",
      "Test Loss: 0.007 | Acc: 92.654%\n",
      "\n",
      "Epoch: 80\n",
      "Train Loss: 0.028 | Acc: 93.531%\n",
      "Test Loss: 0.006 | Acc: 93.489%\n",
      "\n",
      "Epoch: 81\n",
      "Train Loss: 0.028 | Acc: 93.531%\n",
      "Test Loss: 0.007 | Acc: 92.154%\n",
      "\n",
      "Epoch: 82\n",
      "Train Loss: 0.028 | Acc: 93.498%\n",
      "Test Loss: 0.005 | Acc: 93.489%\n",
      "\n",
      "Epoch: 83\n",
      "Train Loss: 0.028 | Acc: 93.364%\n",
      "Test Loss: 0.006 | Acc: 93.155%\n",
      "\n",
      "Epoch: 84\n",
      "Train Loss: 0.027 | Acc: 93.665%\n",
      "Test Loss: 0.006 | Acc: 93.155%\n",
      "\n",
      "Epoch: 85\n",
      "Train Loss: 0.026 | Acc: 93.765%\n",
      "Test Loss: 0.005 | Acc: 93.656%\n",
      "\n",
      "Epoch: 86\n",
      "Train Loss: 0.026 | Acc: 93.765%\n",
      "Test Loss: 0.005 | Acc: 93.322%\n",
      "\n",
      "Epoch: 87\n",
      "Train Loss: 0.026 | Acc: 93.731%\n",
      "Test Loss: 0.005 | Acc: 93.155%\n",
      "\n",
      "Epoch: 88\n",
      "Train Loss: 0.025 | Acc: 93.731%\n",
      "Test Loss: 0.005 | Acc: 93.155%\n",
      "\n",
      "Epoch: 89\n",
      "Train Loss: 0.025 | Acc: 93.731%\n",
      "Test Loss: 0.005 | Acc: 93.656%\n",
      "\n",
      "Epoch: 90\n",
      "Train Loss: 0.025 | Acc: 93.865%\n",
      "Test Loss: 0.006 | Acc: 92.487%\n",
      "\n",
      "Epoch: 91\n",
      "Train Loss: 0.024 | Acc: 93.665%\n",
      "Test Loss: 0.005 | Acc: 93.823%\n",
      "\n",
      "Epoch: 92\n",
      "Train Loss: 0.024 | Acc: 93.698%\n",
      "Test Loss: 0.005 | Acc: 93.155%\n",
      "\n",
      "Epoch: 93\n",
      "Train Loss: 0.024 | Acc: 93.498%\n",
      "Test Loss: 0.005 | Acc: 93.823%\n",
      "\n",
      "Epoch: 94\n",
      "Train Loss: 0.024 | Acc: 93.731%\n",
      "Test Loss: 0.005 | Acc: 93.322%\n",
      "\n",
      "Epoch: 95\n",
      "Train Loss: 0.023 | Acc: 93.831%\n",
      "Test Loss: 0.006 | Acc: 91.987%\n",
      "\n",
      "Epoch: 96\n",
      "Train Loss: 0.023 | Acc: 93.298%\n",
      "Test Loss: 0.005 | Acc: 93.155%\n",
      "\n",
      "Epoch: 97\n",
      "Train Loss: 0.022 | Acc: 93.798%\n",
      "Test Loss: 0.004 | Acc: 93.656%\n",
      "\n",
      "Epoch: 98\n",
      "Train Loss: 0.022 | Acc: 94.465%\n",
      "Test Loss: 0.005 | Acc: 93.823%\n",
      "\n",
      "Epoch: 99\n",
      "Train Loss: 0.022 | Acc: 96.265%\n",
      "Test Loss: 0.004 | Acc: 95.826%\n",
      "\n",
      "Epoch: 100\n",
      "Train Loss: 0.022 | Acc: 97.499%\n",
      "Test Loss: 0.004 | Acc: 96.494%\n",
      "\n",
      "Epoch: 101\n",
      "Train Loss: 0.020 | Acc: 97.999%\n",
      "Test Loss: 0.005 | Acc: 96.661%\n",
      "\n",
      "Epoch: 102\n",
      "Train Loss: 0.021 | Acc: 98.299%\n",
      "Test Loss: 0.004 | Acc: 96.828%\n",
      "\n",
      "Epoch: 103\n",
      "Train Loss: 0.020 | Acc: 98.166%\n",
      "Test Loss: 0.004 | Acc: 96.995%\n",
      "\n",
      "Epoch: 104\n",
      "Train Loss: 0.019 | Acc: 98.399%\n",
      "Test Loss: 0.004 | Acc: 97.329%\n",
      "\n",
      "Epoch: 105\n",
      "Train Loss: 0.020 | Acc: 98.633%\n",
      "Test Loss: 0.004 | Acc: 97.162%\n",
      "\n",
      "Epoch: 106\n",
      "Train Loss: 0.019 | Acc: 98.700%\n",
      "Test Loss: 0.004 | Acc: 96.995%\n",
      "\n",
      "Epoch: 107\n",
      "Train Loss: 0.019 | Acc: 98.666%\n",
      "Test Loss: 0.005 | Acc: 97.162%\n",
      "\n",
      "Epoch: 108\n",
      "Train Loss: 0.018 | Acc: 98.600%\n",
      "Test Loss: 0.004 | Acc: 97.329%\n",
      "\n",
      "Epoch: 109\n",
      "Train Loss: 0.019 | Acc: 98.833%\n",
      "Test Loss: 0.004 | Acc: 97.329%\n",
      "\n",
      "Epoch: 110\n",
      "Train Loss: 0.018 | Acc: 98.800%\n",
      "Test Loss: 0.004 | Acc: 97.329%\n",
      "\n",
      "Epoch: 111\n",
      "Train Loss: 0.017 | Acc: 98.933%\n",
      "Test Loss: 0.004 | Acc: 97.663%\n",
      "\n",
      "Epoch: 112\n",
      "Train Loss: 0.017 | Acc: 98.933%\n",
      "Test Loss: 0.003 | Acc: 97.663%\n",
      "\n",
      "Epoch: 113\n",
      "Train Loss: 0.018 | Acc: 99.100%\n",
      "Test Loss: 0.003 | Acc: 97.663%\n",
      "\n",
      "Epoch: 114\n",
      "Train Loss: 0.016 | Acc: 98.866%\n",
      "Test Loss: 0.004 | Acc: 97.830%\n",
      "\n",
      "Epoch: 115\n",
      "Train Loss: 0.017 | Acc: 98.833%\n",
      "Test Loss: 0.004 | Acc: 97.496%\n",
      "\n",
      "Epoch: 116\n",
      "Train Loss: 0.016 | Acc: 98.833%\n",
      "Test Loss: 0.003 | Acc: 97.496%\n",
      "\n",
      "Epoch: 117\n",
      "Train Loss: 0.015 | Acc: 98.966%\n",
      "Test Loss: 0.004 | Acc: 97.663%\n",
      "\n",
      "Epoch: 118\n",
      "Train Loss: 0.016 | Acc: 98.966%\n",
      "Test Loss: 0.004 | Acc: 97.663%\n",
      "\n",
      "Epoch: 119\n",
      "Train Loss: 0.015 | Acc: 98.933%\n",
      "Test Loss: 0.003 | Acc: 98.164%\n",
      "\n",
      "Epoch: 120\n",
      "Train Loss: 0.015 | Acc: 98.966%\n",
      "Test Loss: 0.004 | Acc: 97.663%\n",
      "\n",
      "Epoch: 121\n",
      "Train Loss: 0.015 | Acc: 99.000%\n",
      "Test Loss: 0.003 | Acc: 97.663%\n",
      "\n",
      "Epoch: 122\n",
      "Train Loss: 0.015 | Acc: 99.100%\n",
      "Test Loss: 0.003 | Acc: 97.663%\n",
      "\n",
      "Epoch: 123\n",
      "Train Loss: 0.015 | Acc: 98.933%\n",
      "Test Loss: 0.003 | Acc: 97.663%\n",
      "\n",
      "Epoch: 124\n",
      "Train Loss: 0.014 | Acc: 99.000%\n",
      "Test Loss: 0.003 | Acc: 97.830%\n",
      "\n",
      "Epoch: 125\n",
      "Train Loss: 0.014 | Acc: 99.133%\n",
      "Test Loss: 0.003 | Acc: 97.663%\n",
      "\n",
      "Epoch: 126\n",
      "Train Loss: 0.014 | Acc: 99.033%\n",
      "Test Loss: 0.004 | Acc: 97.830%\n",
      "\n",
      "Epoch: 127\n",
      "Train Loss: 0.014 | Acc: 99.100%\n",
      "Test Loss: 0.003 | Acc: 97.663%\n",
      "\n",
      "Epoch: 128\n",
      "Train Loss: 0.014 | Acc: 99.066%\n",
      "Test Loss: 0.003 | Acc: 97.830%\n",
      "\n",
      "Epoch: 129\n",
      "Train Loss: 0.014 | Acc: 99.033%\n",
      "Test Loss: 0.003 | Acc: 97.830%\n",
      "\n",
      "Epoch: 130\n",
      "Train Loss: 0.013 | Acc: 99.000%\n",
      "Test Loss: 0.003 | Acc: 98.331%\n",
      "\n",
      "Epoch: 131\n",
      "Train Loss: 0.013 | Acc: 98.933%\n",
      "Test Loss: 0.003 | Acc: 97.997%\n",
      "\n",
      "Epoch: 132\n",
      "Train Loss: 0.013 | Acc: 99.033%\n",
      "Test Loss: 0.003 | Acc: 97.997%\n",
      "\n",
      "Epoch: 133\n",
      "Train Loss: 0.013 | Acc: 99.000%\n",
      "Test Loss: 0.003 | Acc: 97.830%\n",
      "\n",
      "Epoch: 134\n",
      "Train Loss: 0.013 | Acc: 98.966%\n",
      "Test Loss: 0.003 | Acc: 97.997%\n",
      "\n",
      "Epoch: 135\n",
      "Train Loss: 0.013 | Acc: 98.933%\n",
      "Test Loss: 0.003 | Acc: 97.997%\n",
      "\n",
      "Epoch: 136\n",
      "Train Loss: 0.012 | Acc: 99.133%\n",
      "Test Loss: 0.003 | Acc: 97.663%\n",
      "\n",
      "Epoch: 137\n",
      "Train Loss: 0.012 | Acc: 98.966%\n",
      "Test Loss: 0.002 | Acc: 98.664%\n",
      "\n",
      "Epoch: 138\n",
      "Train Loss: 0.012 | Acc: 99.100%\n",
      "Test Loss: 0.004 | Acc: 97.997%\n",
      "\n",
      "Epoch: 139\n",
      "Train Loss: 0.012 | Acc: 99.033%\n",
      "Test Loss: 0.002 | Acc: 97.997%\n",
      "\n",
      "Epoch: 140\n",
      "Train Loss: 0.012 | Acc: 99.000%\n",
      "Test Loss: 0.003 | Acc: 98.164%\n",
      "\n",
      "Epoch: 141\n",
      "Train Loss: 0.012 | Acc: 99.000%\n",
      "Test Loss: 0.002 | Acc: 98.164%\n",
      "\n",
      "Epoch: 142\n",
      "Train Loss: 0.012 | Acc: 99.100%\n",
      "Test Loss: 0.003 | Acc: 98.164%\n",
      "\n",
      "Epoch: 143\n",
      "Train Loss: 0.011 | Acc: 99.033%\n",
      "Test Loss: 0.003 | Acc: 98.164%\n",
      "\n",
      "Epoch: 144\n",
      "Train Loss: 0.011 | Acc: 99.066%\n",
      "Test Loss: 0.003 | Acc: 98.164%\n",
      "\n",
      "Epoch: 145\n",
      "Train Loss: 0.011 | Acc: 99.066%\n",
      "Test Loss: 0.003 | Acc: 98.164%\n",
      "\n",
      "Epoch: 146\n",
      "Train Loss: 0.011 | Acc: 99.100%\n",
      "Test Loss: 0.003 | Acc: 98.164%\n",
      "\n",
      "Epoch: 147\n",
      "Train Loss: 0.011 | Acc: 99.166%\n",
      "Test Loss: 0.003 | Acc: 98.331%\n",
      "\n",
      "Epoch: 148\n",
      "Train Loss: 0.011 | Acc: 99.033%\n",
      "Test Loss: 0.002 | Acc: 98.497%\n",
      "\n",
      "Epoch: 149\n",
      "Train Loss: 0.011 | Acc: 99.066%\n",
      "Test Loss: 0.003 | Acc: 98.331%\n",
      "\n",
      "Epoch: 150\n",
      "Train Loss: 0.011 | Acc: 99.066%\n",
      "Test Loss: 0.002 | Acc: 98.331%\n",
      "\n",
      "Epoch: 151\n",
      "Train Loss: 0.011 | Acc: 99.066%\n",
      "Test Loss: 0.003 | Acc: 98.331%\n",
      "\n",
      "Epoch: 152\n",
      "Train Loss: 0.011 | Acc: 99.200%\n",
      "Test Loss: 0.002 | Acc: 98.331%\n",
      "\n",
      "Epoch: 153\n",
      "Train Loss: 0.010 | Acc: 98.933%\n",
      "Test Loss: 0.003 | Acc: 98.164%\n",
      "\n",
      "Epoch: 154\n",
      "Train Loss: 0.011 | Acc: 99.000%\n",
      "Test Loss: 0.002 | Acc: 98.664%\n",
      "\n",
      "Epoch: 155\n",
      "Train Loss: 0.011 | Acc: 98.866%\n",
      "Test Loss: 0.003 | Acc: 98.164%\n",
      "\n",
      "Epoch: 156\n",
      "Train Loss: 0.010 | Acc: 99.133%\n",
      "Test Loss: 0.002 | Acc: 97.997%\n",
      "\n",
      "Epoch: 157\n",
      "Train Loss: 0.010 | Acc: 99.066%\n",
      "Test Loss: 0.002 | Acc: 98.998%\n",
      "\n",
      "Epoch: 158\n",
      "Train Loss: 0.010 | Acc: 99.100%\n",
      "Test Loss: 0.002 | Acc: 97.997%\n",
      "\n",
      "Epoch: 159\n",
      "Train Loss: 0.010 | Acc: 99.100%\n",
      "Test Loss: 0.002 | Acc: 98.331%\n",
      "\n",
      "Epoch: 160\n",
      "Train Loss: 0.010 | Acc: 99.200%\n",
      "Test Loss: 0.002 | Acc: 98.164%\n",
      "\n",
      "Epoch: 161\n",
      "Train Loss: 0.010 | Acc: 99.033%\n",
      "Test Loss: 0.002 | Acc: 98.331%\n",
      "\n",
      "Epoch: 162\n",
      "Train Loss: 0.009 | Acc: 99.100%\n",
      "Test Loss: 0.002 | Acc: 98.331%\n",
      "\n",
      "Epoch: 163\n",
      "Train Loss: 0.010 | Acc: 99.133%\n",
      "Test Loss: 0.002 | Acc: 98.331%\n",
      "\n",
      "Epoch: 164\n",
      "Train Loss: 0.009 | Acc: 99.166%\n",
      "Test Loss: 0.002 | Acc: 98.164%\n",
      "\n",
      "Epoch: 165\n",
      "Train Loss: 0.010 | Acc: 99.133%\n",
      "Test Loss: 0.002 | Acc: 98.664%\n",
      "\n",
      "Epoch: 166\n",
      "Train Loss: 0.010 | Acc: 99.000%\n",
      "Test Loss: 0.003 | Acc: 98.164%\n",
      "\n",
      "Epoch: 167\n",
      "Train Loss: 0.009 | Acc: 99.133%\n",
      "Test Loss: 0.002 | Acc: 98.164%\n",
      "\n",
      "Epoch: 168\n",
      "Train Loss: 0.009 | Acc: 99.166%\n",
      "Test Loss: 0.002 | Acc: 98.831%\n",
      "\n",
      "Epoch: 169\n",
      "Train Loss: 0.009 | Acc: 99.033%\n",
      "Test Loss: 0.003 | Acc: 98.331%\n",
      "\n",
      "Epoch: 170\n",
      "Train Loss: 0.009 | Acc: 99.200%\n",
      "Test Loss: 0.002 | Acc: 98.331%\n",
      "\n",
      "Epoch: 171\n",
      "Train Loss: 0.009 | Acc: 99.133%\n",
      "Test Loss: 0.002 | Acc: 98.164%\n",
      "\n",
      "Epoch: 172\n",
      "Train Loss: 0.009 | Acc: 99.133%\n",
      "Test Loss: 0.002 | Acc: 98.998%\n",
      "\n",
      "Epoch: 173\n",
      "Train Loss: 0.009 | Acc: 99.100%\n",
      "Test Loss: 0.003 | Acc: 98.164%\n",
      "\n",
      "Epoch: 174\n",
      "Train Loss: 0.009 | Acc: 99.033%\n",
      "Test Loss: 0.002 | Acc: 98.664%\n",
      "\n",
      "Epoch: 175\n",
      "Train Loss: 0.008 | Acc: 99.233%\n",
      "Test Loss: 0.002 | Acc: 98.331%\n",
      "\n",
      "Epoch: 176\n",
      "Train Loss: 0.008 | Acc: 99.166%\n",
      "Test Loss: 0.002 | Acc: 98.331%\n",
      "\n",
      "Epoch: 177\n",
      "Train Loss: 0.009 | Acc: 99.133%\n",
      "Test Loss: 0.002 | Acc: 98.497%\n",
      "\n",
      "Epoch: 178\n",
      "Train Loss: 0.008 | Acc: 99.166%\n",
      "Test Loss: 0.002 | Acc: 98.331%\n",
      "\n",
      "Epoch: 179\n",
      "Train Loss: 0.008 | Acc: 99.233%\n",
      "Test Loss: 0.002 | Acc: 98.331%\n",
      "\n",
      "Epoch: 180\n",
      "Train Loss: 0.009 | Acc: 99.100%\n",
      "Test Loss: 0.003 | Acc: 97.997%\n",
      "\n",
      "Epoch: 181\n",
      "Train Loss: 0.009 | Acc: 99.100%\n",
      "Test Loss: 0.002 | Acc: 98.831%\n",
      "\n",
      "Epoch: 182\n",
      "Train Loss: 0.008 | Acc: 99.166%\n",
      "Test Loss: 0.002 | Acc: 98.331%\n",
      "\n",
      "Epoch: 183\n",
      "Train Loss: 0.008 | Acc: 99.200%\n",
      "Test Loss: 0.002 | Acc: 98.497%\n",
      "\n",
      "Epoch: 184\n",
      "Train Loss: 0.008 | Acc: 99.166%\n",
      "Test Loss: 0.002 | Acc: 98.164%\n",
      "\n",
      "Epoch: 185\n",
      "Train Loss: 0.009 | Acc: 99.133%\n",
      "Test Loss: 0.002 | Acc: 98.497%\n",
      "\n",
      "Epoch: 186\n",
      "Train Loss: 0.008 | Acc: 99.066%\n",
      "Test Loss: 0.002 | Acc: 98.164%\n",
      "\n",
      "Epoch: 187\n",
      "Train Loss: 0.008 | Acc: 99.200%\n",
      "Test Loss: 0.002 | Acc: 98.331%\n",
      "\n",
      "Epoch: 188\n",
      "Train Loss: 0.008 | Acc: 99.266%\n",
      "Test Loss: 0.002 | Acc: 98.497%\n",
      "\n",
      "Epoch: 189\n",
      "Train Loss: 0.008 | Acc: 99.200%\n",
      "Test Loss: 0.002 | Acc: 98.164%\n",
      "\n",
      "Epoch: 190\n",
      "Train Loss: 0.008 | Acc: 99.333%\n",
      "Test Loss: 0.002 | Acc: 98.497%\n",
      "\n",
      "Epoch: 191\n",
      "Train Loss: 0.008 | Acc: 99.166%\n",
      "Test Loss: 0.002 | Acc: 98.331%\n",
      "\n",
      "Epoch: 192\n",
      "Train Loss: 0.008 | Acc: 99.166%\n",
      "Test Loss: 0.003 | Acc: 97.496%\n",
      "\n",
      "Epoch: 193\n",
      "Train Loss: 0.008 | Acc: 99.366%\n",
      "Test Loss: 0.002 | Acc: 98.331%\n",
      "\n",
      "Epoch: 194\n",
      "Train Loss: 0.007 | Acc: 99.233%\n",
      "Test Loss: 0.002 | Acc: 98.331%\n",
      "\n",
      "Epoch: 195\n",
      "Train Loss: 0.008 | Acc: 99.300%\n",
      "Test Loss: 0.002 | Acc: 98.831%\n",
      "\n",
      "Epoch: 196\n",
      "Train Loss: 0.008 | Acc: 99.133%\n",
      "Test Loss: 0.002 | Acc: 98.831%\n",
      "\n",
      "Epoch: 197\n",
      "Train Loss: 0.008 | Acc: 99.233%\n",
      "Test Loss: 0.002 | Acc: 98.497%\n",
      "\n",
      "Epoch: 198\n",
      "Train Loss: 0.007 | Acc: 99.300%\n",
      "Test Loss: 0.002 | Acc: 98.164%\n",
      "\n",
      "Epoch: 199\n",
      "Train Loss: 0.007 | Acc: 99.333%\n",
      "Test Loss: 0.002 | Acc: 98.497%\n",
      "\n",
      "Epoch: 200\n",
      "Train Loss: 0.007 | Acc: 99.200%\n",
      "Test Loss: 0.002 | Acc: 97.997%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import argparse\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import os, sys\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"\n",
    "os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"true\"\n",
    "\n",
    "# m1: no LA\n",
    "# m2: FC -> GAP\n",
    "# m3: no DFT\n",
    "model_scheme = \"m123\" # org, m1, m12, m123\n",
    "n_classes = 6\n",
    "\n",
    "class Args(object):\n",
    "    def __init__(self, n_epochs, lr, n_classes, n_samples, distance):\n",
    "        self.n_epochs = n_epochs\n",
    "        self.lr = lr\n",
    "        self.n_classes = n_classes\n",
    "        self.n_samples = n_samples\n",
    "        self.distance = distance\n",
    "\n",
    "# Make custom dataset\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, images, labels):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        image = torch.FloatTensor(self.images[idx])\n",
    "        # image = (image + 120.2075) / 20.1417\n",
    "        label = int(self.labels[idx])\n",
    "        return image, label\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.images.shape[0]\n",
    "    \n",
    "\n",
    "# Define the network (fc)\n",
    "class CNN_org(nn.Module):\n",
    "    def __init__(self, n_classes=12):\n",
    "        super(CNN_org, self).__init__()\n",
    "        self.n_classes = n_classes\n",
    "        self.conv1 = nn.Conv2d(1, 5, kernel_size=5, padding=0)\n",
    "        self.pool1 = nn.AvgPool2d(kernel_size=4, stride=4)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(5, 10, kernel_size=5, padding=0)\n",
    "        self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(200, n_classes)  # original -> no padding, 200 / M1 -> padding, 240\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = x.view(-1, 200)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "    \n",
    "# Define the network (fc)\n",
    "class CNN_m1(nn.Module):\n",
    "    def __init__(self, n_classes=12):\n",
    "        super(CNN_m1, self).__init__()\n",
    "        self.n_classes = n_classes\n",
    "        self.conv1 = nn.Conv2d(1, 5, kernel_size=5, padding=2)\n",
    "        self.pool1 = nn.AvgPool2d(kernel_size=4, stride=4)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(5, 10, kernel_size=5, padding=2)\n",
    "        self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(240, n_classes)  # original -> no padding, 200 / M1 -> padding, 240\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = x.view(-1, 240)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "# Define the network (GAP)\n",
    "class CNN_m123(nn.Module):\n",
    "    def __init__(self, n_classes=12):\n",
    "        super(CNN_m123, self).__init__()\n",
    "        self.n_classes = n_classes\n",
    "        self.conv1 = nn.Conv2d(1, 5, kernel_size=5, padding=2)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=4, stride=4)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(5, 10, kernel_size=5, padding=2)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.features = nn.Conv2d(10, n_classes, kernel_size=3, padding=1)\n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.features(x)\n",
    "        x = self.gap(x)\n",
    "        x = x.view(-1, self.n_classes)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "args = Args(n_epochs=200, lr=1e-4, n_classes=6, n_samples=100, distance=100)\n",
    "    \n",
    "n_samples = args.n_samples\n",
    "sample_distance = args.distance\n",
    "\n",
    "# Load activity image data\n",
    "if(model_scheme == \"org\"):\n",
    "    images = np.load('./real-data/real_images_100_100_6_0826_LinearAcc_AI.npy')#.format(n_samples, sample_distance, args.n_classes))\n",
    "    labels = np.load('./real-data/real_labels_100_100_6_0826_LinearAcc_AI.npy')#.format(n_samples, sample_distance, args.n_classes))\n",
    "elif(model_scheme == \"m1\"):\n",
    "    images = np.load('./real-data/real_images_100_100_6_0826_M1.npy')#.format(n_samples, sample_distance, args.n_classes))\n",
    "    labels = np.load('./real-data/real_labels_100_100_6_0826_M1.npy')#.format(n_samples, sample_distance, args.n_classes))\n",
    "elif(model_scheme == \"m12\"):\n",
    "    images = np.load('./real-data/real_images_100_100_6_0826_M1+M2+M3.npy')#.format(n_samples, sample_distance, args.n_classes))\n",
    "    labels = np.load('./real-data/real_labels_100_100_6_0826_M1+M2+M3.npy')#.format(n_samples, sample_distance, args.n_classes))\n",
    "elif(model_scheme == \"m123\"):\n",
    "    images = np.load('./real-data/real_images_100_100_6_0826_M1+M2+M3.npy')#.format(n_samples, sample_distance, args.n_classes))\n",
    "    labels = np.load('./real-data/real_labels_100_100_6_0826_M1+M2+M3.npy')#.format(n_samples, sample_distance, args.n_classes))\n",
    "\n",
    "# Data\n",
    "dataset = MyDataset(images=images, labels=labels)\n",
    "# mean, std = get_mean_and_std(dataset)\n",
    "# print('size: {}, mean: {}, std: {}'.format(len(dataset), mean, std))\n",
    "split_ratio = 1/6\n",
    "dataset_size = len(dataset)\n",
    "\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(split_ratio * dataset_size))\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(indices)\n",
    "train_indices, test_indices = indices[split:], indices[:split]\n",
    "\n",
    "train_sampler = torch.utils.data.sampler.SubsetRandomSampler(train_indices)\n",
    "test_sampler = torch.utils.data.sampler.SubsetRandomSampler(test_indices)\n",
    "\n",
    "train_loader = DataLoader(dataset=dataset, batch_size=128, sampler=train_sampler)\n",
    "test_loader = DataLoader(dataset=dataset, batch_size=128, sampler=test_sampler)\n",
    "\n",
    "# Parameters\n",
    "use_cuda = torch.cuda.is_available()\n",
    "n_epochs = args.n_epochs\n",
    "learning_rate = args.lr\n",
    "n_classes = args.n_classes\n",
    "\n",
    "\n",
    "# Model\n",
    "if(model_scheme == \"org\"):\n",
    "    net = CNN_org(n_classes)\n",
    "elif(model_scheme == \"m1\"):\n",
    "    net = CNN_m1(n_classes)\n",
    "elif(model_scheme == \"m12\"):\n",
    "    net = CNN_m123(n_classes)\n",
    "elif(model_scheme == \"m123\"):\n",
    "    net = CNN_m123(n_classes)\n",
    "\n",
    "if use_cuda:\n",
    "    net.cuda()\n",
    "\n",
    "# Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-4)\n",
    "# optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=5e-4)\n",
    "\n",
    "# LR scheduler\n",
    "milestones = []\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer=optimizer, milestones=milestones, gamma=0.1)\n",
    "\n",
    "print(\"==> Training..\")\n",
    "\n",
    "# Training\n",
    "for epoch in range(n_epochs):\n",
    "    print('Epoch: {}'.format(epoch + 1))\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct_tr = 0\n",
    "    total_tr = 0\n",
    "\n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        if use_cuda:\n",
    "            images, labels = images.cuda(), labels.cuda()\n",
    "        images = images.unsqueeze(1)\n",
    "        optimizer.zero_grad()\n",
    "        images, labels = Variable(images), Variable(labels)\n",
    "        outputs = net(images)\n",
    "        loss_tr = criterion(outputs, labels)\n",
    "        loss_tr.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss_tr.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_tr += labels.size(0)\n",
    "        correct_tr += predicted.eq(labels.data).cpu().sum()\n",
    "\n",
    "    print('Train Loss: %.3f | Acc: %.3f%%'% (train_loss/128, 100.*correct_tr/total_tr))\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    # Test\n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        test_loss = 0\n",
    "        correct_test = 0\n",
    "        total_test = 0\n",
    "\n",
    "        for batch_idx, (images, labels) in enumerate(test_loader):\n",
    "            if use_cuda:\n",
    "                images, labels = images.cuda(), labels.cuda()\n",
    "            images = images.unsqueeze(1)\n",
    "            images, labels = Variable(images), Variable(labels)\n",
    "            outputs = net(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_test += labels.size(0)\n",
    "            correct_test += predicted.eq(labels.data).cpu().sum()\n",
    "\n",
    "        print('Test Loss: %.3f | Acc: %.3f%%\\n' % (test_loss/128, 100.*correct_test/total_test))\n",
    "\n",
    "torch.save(net.state_dict(), './pretrained-real-data/MyModel.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7471264367816093\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(precision=3)\n",
    "print(np.mean(labels.cpu().detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3598, 16, 100, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow import keras\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if(model_scheme == \"org\"):\n",
    "    print(\"ORG\")\n",
    "    data = np.load('real-data/real_images_100_100_6_0826_LinearAcc_AI.npy')\n",
    "    label = np.load('real-data/real_labels_100_100_6_0826_LinearAcc_AI.npy')\n",
    "elif(model_scheme == \"m1\"):\n",
    "    data = np.load('real-data/real_images_100_100_6_0826_M1.npy')\n",
    "    label = np.load('real-data/real_labels_100_100_6_0826_M1.npy')\n",
    "elif(model_scheme == \"m12\"):\n",
    "    data = np.load('real-data/real_images_100_100_6_0826_M1.npy')\n",
    "    label = np.load('real-data/real_labels_100_100_6_0826_M1.npy')\n",
    "elif(model_scheme == \"m123\"):\n",
    "    data = np.load('real-data/real_images_100_100_6_0826_M1+M2+M3.npy')\n",
    "    label = np.load('real-data/real_labels_100_100_6_0826_M1+M2+M3.npy')\n",
    "\n",
    "data = data[..., np.newaxis]\n",
    "print(data.shape)    \n",
    "\n",
    "train_data, test_data, train_label, test_label = train_test_split(data, label, test_size = 0.01, shuffle = True)\n",
    "\n",
    "##############################  Need more Intuition and study ###############################\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_data, train_label))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1000000).batch(128)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_data, test_label))\n",
    "test_dataset = test_dataset.shuffle(buffer_size=1000000).batch(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"CNN_m123\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         [(None, 16, 100, 1)]      0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 16, 100, 5)        130       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 4, 25, 5)          0         \n",
      "_________________________________________________________________\n",
      "re_lu_14 (ReLU)              (None, 4, 25, 5)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 4, 25, 10)         1260      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 2, 12, 10)         0         \n",
      "_________________________________________________________________\n",
      "re_lu_15 (ReLU)              (None, 2, 12, 10)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 2, 12, 6)          546       \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 6)                 0         \n",
      "=================================================================\n",
      "Total params: 1,936\n",
      "Trainable params: 1,936\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "CNN_m123(\n",
      "  (conv1): Conv2d(1, 5, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (pool1): MaxPool2d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
      "  (relu1): ReLU()\n",
      "  (conv2): Conv2d(5, 10, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (relu2): ReLU()\n",
      "  (features): Conv2d(10, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (gap): AdaptiveAvgPool2d(output_size=1)\n",
      ")\n",
      "Number of trainable params: 1936\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if(model_scheme == \"org\"):\n",
    "    inputs = tf.keras.Input(shape=(36,100,1))\n",
    "    x = layers.Conv2D(5, (5,5))(inputs)\n",
    "    x = layers.AveragePooling2D((4,4))(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv2D(10,(5,5))(x)\n",
    "    x = layers.AveragePooling2D((2,2))(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(n_classes, activation='softmax')(x)\n",
    "    outputs = x\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs, name=\"CNN_org\")\n",
    "    \n",
    "elif(model_scheme == \"m1\"):\n",
    "    inputs = tf.keras.Input(shape=(16,100,1))\n",
    "    x = layers.Conv2D(5, (5,5), padding='same')(inputs)\n",
    "    x = layers.AveragePooling2D((4,4))(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv2D(10,(5,5), padding='same')(x)\n",
    "    x = layers.AveragePooling2D((2,2))(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x= layers.Dense(n_classes, activation='softmax')(x)\n",
    "    outputs = x\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs, name=\"CNN_m1\")\n",
    "    \n",
    "elif(model_scheme == \"m12\"):\n",
    "    inputs = tf.keras.Input(shape=(16,100,1))\n",
    "    x = layers.Conv2D(5, (5,5), padding='same')(inputs)\n",
    "    x = layers.MaxPool2D((4,4))(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv2D(10,(5,5), padding='same')(x)\n",
    "    x = layers.MaxPool2D((2,2))(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv2D(n_classes,(3,3), padding='same')(x)\n",
    "    outputs = layers.GlobalAveragePooling2D()(x)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs, name=\"CNN_m12\")\n",
    "    \n",
    "elif(model_scheme == \"m123\"):\n",
    "    inputs = tf.keras.Input(shape=(16,100,1))\n",
    "    x = layers.Conv2D(5, (5,5), padding='same')(inputs)\n",
    "    x = layers.MaxPool2D((4,4))(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv2D(10,(5,5), padding='same')(x)\n",
    "    x = layers.MaxPool2D((2,2))(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv2D(n_classes,(3,3), padding='same')(x)\n",
    "    outputs = layers.GlobalAveragePooling2D()(x)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs, name=\"CNN_m123\")\n",
    "    \n",
    "model.summary()\n",
    "print(net)\n",
    "\n",
    "pytorch_total_trainable_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "print(\"Number of trainable params: {}\".format(pytorch_total_trainable_params))\n",
    "\n",
    "net.eval()\n",
    "net.cpu()\n",
    "\n",
    "m = {}\n",
    "for k, v in net.named_parameters():\n",
    "    m[k] = v\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-b9eaf93792e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#print(m['fc1.weight'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#print(m['conv2.weight'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "#print(m['fc1.weight'])\n",
    "print(model.layers[8].weights[0].shape)\n",
    "#print(m['conv2.weight'])\n",
    "print(model.layers[4].weights[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(model_scheme == \"org\"):\n",
    "    model.layers[1].set_weights([m['conv1.weight'].permute(2,3,1,0).detach(), m['conv1.bias'].detach()])\n",
    "    model.layers[4].set_weights([m['conv2.weight'].permute(2,3,1,0).detach(), m['conv2.bias'].detach()])\n",
    "    model.layers[8].set_weights([m['fc1.weight'].permute(1,0).detach(), m['fc1.bias'].detach()])\n",
    "elif(model_scheme == \"m1\"):\n",
    "    model.layers[1].set_weights([m['conv1.weight'].permute(2,3,1,0).detach(), m['conv1.bias'].detach()])\n",
    "    model.layers[4].set_weights([m['conv2.weight'].permute(2,3,1,0).detach(), m['conv2.bias'].detach()])\n",
    "    model.layers[8].set_weights([m['fc1.weight'].permute(1,0).detach(), m['fc1.bias'].detach()])\n",
    "elif(model_scheme == \"m12\"):\n",
    "    model.layers[1].set_weights([m['conv1.weight'].permute(2,3,1,0).detach(), m['conv1.bias'].detach()])\n",
    "    model.layers[4].set_weights([m['conv2.weight'].permute(2,3,1,0).detach(), m['conv2.bias'].detach()])\n",
    "    model.layers[7].set_weights([m['features.weight'].permute(2,3,1,0).detach(), m['features.bias'].detach()])\n",
    "elif(model_scheme == \"m123\"):\n",
    "    model.layers[1].set_weights([m['conv1.weight'].permute(2,3,1,0).detach(), m['conv1.bias'].detach()])\n",
    "    model.layers[4].set_weights([m['conv2.weight'].permute(2,3,1,0).detach(), m['conv2.bias'].detach()])\n",
    "    model.layers[7].set_weights([m['features.weight'].permute(2,3,1,0).detach(), m['features.bias'].detach()])\n",
    "\n",
    "\n",
    "#print(m['fc1.weight'])\n",
    "#print(model.layers[8].weights[0])\n",
    "#print(m['conv2.weight'])\n",
    "#print(model.layers[4].weights[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x151b140648c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x151b140648c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-6.103066   4.1064706 -5.2184844 11.045036  -8.990432  -5.0973134]\n",
      "tensor([ 17.4477,   5.6252,   4.4246,  -3.6767,   6.7924, -20.1789],\n",
      "       grad_fn=<SelectBackward>)\n",
      "\n",
      "[-4.1072245  5.6906652 -3.9324636  8.500538  -7.1065555 -3.3163145]\n",
      "tensor([-4.5464, -1.1724,  9.0229,  9.0291, -3.3770,  1.5926],\n",
      "       grad_fn=<SelectBackward>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "    images = images.unsqueeze(1)\n",
    "#    print(\"Torch Data Shape: \",images.shape)\n",
    "    tho = net(images)\n",
    "    \n",
    "    images = images.permute(0,2,3,1).numpy()\n",
    "#    print(\"Torch Permuted Data Shape: \",images.shape)\n",
    "    tfo = model.predict(images)\n",
    "    \n",
    "    print(tfo[0])\n",
    "    print(tho[0])\n",
    "    print()\n",
    "    break\n",
    "\n",
    "\n",
    "for ds in test_dataset.take(1):\n",
    "    images = ds[0]\n",
    "#    print(\"TF Data Shape: \", images.shape)\n",
    "    tfo = model.predict(images)\n",
    "    \n",
    "    images = torch.Tensor(images.numpy()).permute(0,3,1,2)\n",
    "    images.requires_grad = False\n",
    "#    print(\"TF Permuted Data Shape: \", images.shape)\n",
    "    tho = net(images)\n",
    "    \n",
    "    print(tfo[0])\n",
    "    print(tho[0])\n",
    "    print()\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpkzzv1kgq/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpkzzv1kgq/assets\n"
     ]
    }
   ],
   "source": [
    "# Convert the model.\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the TF Lite model.\n",
    "with tf.io.gfile.GFile('model_m123_quant.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.999999747378752e-05\n",
      "Epoch 1/500\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 10.8690 - accuracy: 0.1146 - val_loss: 11.9876 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 2/500\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 10.7930 - accuracy: 0.1123 - val_loss: 11.9456 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 3/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.9460 - accuracy: 0.1070 - val_loss: 11.9013 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 4/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.5610 - accuracy: 0.1184 - val_loss: 11.6629 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 5/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.0978 - accuracy: 0.1125 - val_loss: 11.6472 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 6/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.3010 - accuracy: 0.1125 - val_loss: 11.6310 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 7/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.4892 - accuracy: 0.1147 - val_loss: 11.6138 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 8/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.3751 - accuracy: 0.1071 - val_loss: 11.5992 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 9/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.3400 - accuracy: 0.1072 - val_loss: 11.5883 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 10/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.5429 - accuracy: 0.1100 - val_loss: 11.5798 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 11/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3989 - accuracy: 0.1145 - val_loss: 11.5687 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 12/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.4270 - accuracy: 0.1177 - val_loss: 11.5629 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 13/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.2955 - accuracy: 0.1184 - val_loss: 11.5537 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 14/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.3336 - accuracy: 0.1134 - val_loss: 11.5509 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 15/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.3708 - accuracy: 0.1128 - val_loss: 11.5498 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 16/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3570 - accuracy: 0.1180 - val_loss: 11.5483 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 17/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.5501 - accuracy: 0.1104 - val_loss: 11.5477 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 18/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.4876 - accuracy: 0.1081 - val_loss: 11.5611 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 19/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.4093 - accuracy: 0.1083 - val_loss: 11.5479 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 20/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.5545 - accuracy: 0.1061 - val_loss: 11.5411 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 21/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.5174 - accuracy: 0.1028 - val_loss: 11.5413 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 22/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3473 - accuracy: 0.1163 - val_loss: 11.5406 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 23/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.4952 - accuracy: 0.1078 - val_loss: 11.5396 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 24/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.3561 - accuracy: 0.1194 - val_loss: 11.5408 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 25/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.4652 - accuracy: 0.1073 - val_loss: 11.5392 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 26/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.5368 - accuracy: 0.1032 - val_loss: 11.5377 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 27/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 10.3492 - accuracy: 0.1092 - val_loss: 11.5389 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 28/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.2909 - accuracy: 0.1078 - val_loss: 11.5376 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 29/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3599 - accuracy: 0.1135 - val_loss: 11.5400 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 30/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.5118 - accuracy: 0.1100 - val_loss: 11.5386 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 31/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.4336 - accuracy: 0.1108 - val_loss: 11.5392 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 32/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.4419 - accuracy: 0.1082 - val_loss: 11.5419 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 33/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.5894 - accuracy: 0.1072 - val_loss: 11.5370 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 34/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.2313 - accuracy: 0.1117 - val_loss: 11.5360 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 35/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3507 - accuracy: 0.1074 - val_loss: 11.5358 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 36/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.2147 - accuracy: 0.1091 - val_loss: 11.5357 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 37/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.4502 - accuracy: 0.1128 - val_loss: 11.5390 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 38/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.4440 - accuracy: 0.1097 - val_loss: 11.5354 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 39/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.4234 - accuracy: 0.1050 - val_loss: 11.5353 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 40/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.4018 - accuracy: 0.1048 - val_loss: 11.5362 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 41/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.4953 - accuracy: 0.1108 - val_loss: 11.5370 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 42/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.4285 - accuracy: 0.1069 - val_loss: 11.5358 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 43/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.6147 - accuracy: 0.1020 - val_loss: 11.5360 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 44/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.2820 - accuracy: 0.1035 - val_loss: 11.5358 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 45/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.1515 - accuracy: 0.1098 - val_loss: 11.5361 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 46/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.6022 - accuracy: 0.1094 - val_loss: 11.5370 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 47/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.3618 - accuracy: 0.1083 - val_loss: 11.5373 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 48/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3501 - accuracy: 0.1051 - val_loss: 11.5346 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 49/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.4068 - accuracy: 0.1033 - val_loss: 11.5358 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 50/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.2371 - accuracy: 0.1084 - val_loss: 11.5357 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 51/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.2124 - accuracy: 0.1132 - val_loss: 11.5350 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 52/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.5855 - accuracy: 0.1109 - val_loss: 11.5360 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 53/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.1940 - accuracy: 0.1158 - val_loss: 11.5336 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 54/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.3460 - accuracy: 0.1195 - val_loss: 11.5357 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 55/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.3456 - accuracy: 0.1065 - val_loss: 11.5343 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 56/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.4379 - accuracy: 0.1124 - val_loss: 11.5353 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 57/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.4464 - accuracy: 0.1031 - val_loss: 11.5357 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 58/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.2218 - accuracy: 0.1192 - val_loss: 11.5348 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 59/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.5004 - accuracy: 0.1078 - val_loss: 11.5344 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 60/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.0361 - accuracy: 0.1103 - val_loss: 11.5338 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 61/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.6239 - accuracy: 0.1046 - val_loss: 11.5349 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 62/500\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 10.2368 - accuracy: 0.1109 - val_loss: 11.5348 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 63/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.4836 - accuracy: 0.1083 - val_loss: 11.5333 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 64/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.2178 - accuracy: 0.1157 - val_loss: 11.5350 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 65/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3204 - accuracy: 0.1083 - val_loss: 11.5354 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 66/500\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 10.2908 - accuracy: 0.1062 - val_loss: 11.5365 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 67/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.2144 - accuracy: 0.1126 - val_loss: 11.5350 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 68/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.4645 - accuracy: 0.1052 - val_loss: 11.5340 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 69/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.4497 - accuracy: 0.1050 - val_loss: 11.5948 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 70/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.2817 - accuracy: 0.1216 - val_loss: 11.5461 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 71/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.4330 - accuracy: 0.1065 - val_loss: 11.5405 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 72/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.6144 - accuracy: 0.1085 - val_loss: 11.5338 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 73/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.4114 - accuracy: 0.1024 - val_loss: 11.5324 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 74/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.4307 - accuracy: 0.1061 - val_loss: 11.5323 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 75/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.4351 - accuracy: 0.1166 - val_loss: 11.5342 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 76/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3000 - accuracy: 0.0996 - val_loss: 11.5981 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 77/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.4572 - accuracy: 0.1116 - val_loss: 11.5416 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 78/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.4657 - accuracy: 0.1030 - val_loss: 11.5340 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 79/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.5858 - accuracy: 0.1062 - val_loss: 11.5297 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 80/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.7783 - accuracy: 0.1004 - val_loss: 11.5337 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 81/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.2226 - accuracy: 0.1082 - val_loss: 11.6065 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 82/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.4789 - accuracy: 0.1075 - val_loss: 11.5338 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 83/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.4090 - accuracy: 0.1038 - val_loss: 11.5323 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 84/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.2784 - accuracy: 0.1028 - val_loss: 11.5354 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 85/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.5419 - accuracy: 0.1051 - val_loss: 11.5326 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 86/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3879 - accuracy: 0.1121 - val_loss: 11.5356 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 87/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.5283 - accuracy: 0.1101 - val_loss: 11.5323 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 88/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.7237 - accuracy: 0.1046 - val_loss: 11.5357 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 89/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.2656 - accuracy: 0.1094 - val_loss: 11.5322 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 90/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.6089 - accuracy: 0.0970 - val_loss: 11.5333 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 91/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.5104 - accuracy: 0.1056 - val_loss: 11.5333 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 92/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3309 - accuracy: 0.1109 - val_loss: 11.5333 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 93/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.5334 - accuracy: 0.1135 - val_loss: 11.5349 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 94/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.4420 - accuracy: 0.1046 - val_loss: 11.5324 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 95/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3031 - accuracy: 0.1030 - val_loss: 11.5318 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 96/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.4384 - accuracy: 0.1118 - val_loss: 11.5323 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 97/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.5014 - accuracy: 0.1015 - val_loss: 11.5335 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 98/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3114 - accuracy: 0.1073 - val_loss: 11.5304 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 99/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.2412 - accuracy: 0.1147 - val_loss: 11.5346 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 100/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3154 - accuracy: 0.1179 - val_loss: 11.5327 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 101/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3995 - accuracy: 0.1175 - val_loss: 11.5313 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 102/500\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 10.4340 - accuracy: 0.1150 - val_loss: 11.5320 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 103/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3978 - accuracy: 0.1037 - val_loss: 11.5350 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 104/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.2786 - accuracy: 0.1095 - val_loss: 11.5338 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 105/500\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 10.3580 - accuracy: 0.1052 - val_loss: 11.5338 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 106/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.4784 - accuracy: 0.1082 - val_loss: 11.5325 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 107/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.3928 - accuracy: 0.1107 - val_loss: 11.5354 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 108/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.1827 - accuracy: 0.1097 - val_loss: 11.5312 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 109/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.2947 - accuracy: 0.1128 - val_loss: 11.5304 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 110/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.5657 - accuracy: 0.1088 - val_loss: 11.5330 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 111/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.2719 - accuracy: 0.1112 - val_loss: 11.5338 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 112/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3836 - accuracy: 0.1114 - val_loss: 11.5332 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 113/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 10.2904 - accuracy: 0.1084 - val_loss: 11.5340 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 114/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.3240 - accuracy: 0.1113 - val_loss: 11.5324 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 115/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.4027 - accuracy: 0.1117 - val_loss: 11.5328 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 116/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 10.3449 - accuracy: 0.1051 - val_loss: 11.5306 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 117/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 10.3869 - accuracy: 0.1098 - val_loss: 11.5316 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 118/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 10.2024 - accuracy: 0.1163 - val_loss: 11.5328 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 119/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.4079 - accuracy: 0.1115 - val_loss: 11.5343 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 120/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.3747 - accuracy: 0.1126 - val_loss: 11.5306 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 121/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.4556 - accuracy: 0.1101 - val_loss: 11.5322 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 122/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.4948 - accuracy: 0.1025 - val_loss: 11.5311 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 123/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3763 - accuracy: 0.1141 - val_loss: 11.5343 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 124/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.6168 - accuracy: 0.1008 - val_loss: 11.5305 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 125/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.2432 - accuracy: 0.1172 - val_loss: 11.5323 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 126/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3343 - accuracy: 0.1087 - val_loss: 11.5315 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 127/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.5516 - accuracy: 0.1085 - val_loss: 11.5320 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 128/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.3430 - accuracy: 0.1074 - val_loss: 11.5302 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 129/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.4126 - accuracy: 0.1066 - val_loss: 11.5308 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 130/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3966 - accuracy: 0.1057 - val_loss: 11.5299 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 131/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3323 - accuracy: 0.1085 - val_loss: 11.5326 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 132/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.4073 - accuracy: 0.1059 - val_loss: 11.5305 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 133/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.2605 - accuracy: 0.1090 - val_loss: 11.5337 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 134/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3452 - accuracy: 0.1157 - val_loss: 11.5296 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 135/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.5149 - accuracy: 0.1069 - val_loss: 11.5289 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 136/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 10.4786 - accuracy: 0.1158 - val_loss: 11.5349 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 137/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.0490 - accuracy: 0.1119 - val_loss: 11.5300 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 138/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.1689 - accuracy: 0.1104 - val_loss: 11.5295 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 139/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.4337 - accuracy: 0.1092 - val_loss: 11.5296 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 140/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.4213 - accuracy: 0.1091 - val_loss: 11.5311 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 141/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.3858 - accuracy: 0.1128 - val_loss: 11.5298 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 142/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.3755 - accuracy: 0.1080 - val_loss: 11.5288 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 143/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3706 - accuracy: 0.1128 - val_loss: 11.5300 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 144/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3824 - accuracy: 0.1057 - val_loss: 11.5311 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 145/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3497 - accuracy: 0.1064 - val_loss: 11.5304 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 146/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.4181 - accuracy: 0.1130 - val_loss: 11.5292 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 147/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3456 - accuracy: 0.1124 - val_loss: 11.5294 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 148/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3177 - accuracy: 0.1090 - val_loss: 11.5305 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 149/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.4127 - accuracy: 0.1073 - val_loss: 11.5298 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 150/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.3731 - accuracy: 0.1107 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 151/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.4050 - accuracy: 0.1058 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 152/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.2299 - accuracy: 0.1125 - val_loss: 11.5289 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 153/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.4794 - accuracy: 0.1113 - val_loss: 11.5288 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 154/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3445 - accuracy: 0.1145 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 155/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.4914 - accuracy: 0.1027 - val_loss: 11.5290 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 156/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.6059 - accuracy: 0.1109 - val_loss: 11.5298 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 157/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.3764 - accuracy: 0.1101 - val_loss: 11.5301 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 158/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.1739 - accuracy: 0.1177 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 159/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 10.4214 - accuracy: 0.1109 - val_loss: 11.5289 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 160/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.4403 - accuracy: 0.1086 - val_loss: 11.5294 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 161/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.3039 - accuracy: 0.1098 - val_loss: 11.5288 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 162/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.3379 - accuracy: 0.1148 - val_loss: 11.5290 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 163/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.3534 - accuracy: 0.1089 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 164/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.5417 - accuracy: 0.1047 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 165/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.5147 - accuracy: 0.1046 - val_loss: 11.5287 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 166/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 10.3145 - accuracy: 0.1135 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 167/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3990 - accuracy: 0.1097 - val_loss: 11.5302 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 168/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.4492 - accuracy: 0.1043 - val_loss: 11.5352 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 169/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.5537 - accuracy: 0.0994 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 170/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3424 - accuracy: 0.1169 - val_loss: 11.5298 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 171/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.4297 - accuracy: 0.1081 - val_loss: 11.5291 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 172/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.3122 - accuracy: 0.1095 - val_loss: 11.5309 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 173/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.2108 - accuracy: 0.1090 - val_loss: 11.5332 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 174/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3628 - accuracy: 0.1084 - val_loss: 11.5299 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 175/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.1039 - accuracy: 0.1073 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 176/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.5112 - accuracy: 0.1081 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 177/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.3616 - accuracy: 0.1062 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 178/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.4390 - accuracy: 0.0979 - val_loss: 11.5306 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 179/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.2967 - accuracy: 0.1056 - val_loss: 11.5297 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 180/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.4922 - accuracy: 0.1085 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 181/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.3200 - accuracy: 0.1055 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 182/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.6104 - accuracy: 0.1035 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 183/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.4442 - accuracy: 0.1127 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 184/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.2597 - accuracy: 0.1046 - val_loss: 11.5290 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 185/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.4991 - accuracy: 0.1153 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 186/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.5714 - accuracy: 0.1051 - val_loss: 11.5291 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 187/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3174 - accuracy: 0.1105 - val_loss: 11.5323 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 188/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.3401 - accuracy: 0.1107 - val_loss: 11.5300 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 189/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.2853 - accuracy: 0.1095 - val_loss: 11.5298 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 190/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.5246 - accuracy: 0.1029 - val_loss: 11.5292 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 191/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.2954 - accuracy: 0.1159 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 192/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.4068 - accuracy: 0.1089 - val_loss: 11.5290 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 193/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3032 - accuracy: 0.1107 - val_loss: 11.5308 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 194/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.5732 - accuracy: 0.1039 - val_loss: 11.5293 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 195/500\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 10.2660 - accuracy: 0.1104 - val_loss: 11.5295 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 196/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.4238 - accuracy: 0.1026 - val_loss: 11.5291 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 197/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3861 - accuracy: 0.1159 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 198/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.2630 - accuracy: 0.1137 - val_loss: 11.5298 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 199/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.3727 - accuracy: 0.1111 - val_loss: 11.5300 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 200/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3387 - accuracy: 0.1114 - val_loss: 11.5296 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 201/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3649 - accuracy: 0.1013 - val_loss: 11.5309 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 202/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.5574 - accuracy: 0.1019 - val_loss: 11.5293 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 203/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3507 - accuracy: 0.1142 - val_loss: 11.5288 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 204/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.4304 - accuracy: 0.1051 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 205/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3023 - accuracy: 0.1040 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 206/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3674 - accuracy: 0.1029 - val_loss: 11.5290 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 207/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.4658 - accuracy: 0.1103 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 208/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.4989 - accuracy: 0.1067 - val_loss: 11.5297 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 209/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.4244 - accuracy: 0.1091 - val_loss: 11.5315 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 210/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.4768 - accuracy: 0.1084 - val_loss: 11.5298 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 211/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3548 - accuracy: 0.1087 - val_loss: 11.5297 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 212/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3601 - accuracy: 0.1084 - val_loss: 11.5310 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 213/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.4019 - accuracy: 0.1137 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 214/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.0400 - accuracy: 0.1192 - val_loss: 11.5299 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 215/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.4055 - accuracy: 0.1120 - val_loss: 11.5333 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 216/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.0910 - accuracy: 0.1114 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 217/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.5131 - accuracy: 0.1027 - val_loss: 11.5312 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 218/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3004 - accuracy: 0.1149 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 219/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.2380 - accuracy: 0.1082 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 220/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.4128 - accuracy: 0.1003 - val_loss: 11.5303 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 221/500\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 10.4373 - accuracy: 0.1043 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 222/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.2706 - accuracy: 0.1104 - val_loss: 11.5299 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 223/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.3243 - accuracy: 0.1084 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 224/500\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 10.3412 - accuracy: 0.1078 - val_loss: 11.5294 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 225/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.3552 - accuracy: 0.1151 - val_loss: 11.5290 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 226/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.5412 - accuracy: 0.1096 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 227/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.4272 - accuracy: 0.1162 - val_loss: 11.5290 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 228/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.2175 - accuracy: 0.1203 - val_loss: 11.5291 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 229/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.2338 - accuracy: 0.1101 - val_loss: 11.5298 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 230/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.4893 - accuracy: 0.1054 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 231/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.3313 - accuracy: 0.1113 - val_loss: 11.5287 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 232/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.3531 - accuracy: 0.1038 - val_loss: 11.5316 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 233/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.2511 - accuracy: 0.1097 - val_loss: 11.5287 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 234/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.4628 - accuracy: 0.1109 - val_loss: 11.5289 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 235/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.3496 - accuracy: 0.1091 - val_loss: 11.5303 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 236/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.4214 - accuracy: 0.1013 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 237/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3379 - accuracy: 0.1016 - val_loss: 11.5304 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 238/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.4172 - accuracy: 0.1090 - val_loss: 11.5287 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 239/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.1641 - accuracy: 0.1177 - val_loss: 11.5296 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 240/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 10.4871 - accuracy: 0.1131 - val_loss: 11.5296 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 241/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.4356 - accuracy: 0.1082 - val_loss: 11.5299 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 242/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.1517 - accuracy: 0.1140 - val_loss: 11.5295 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 243/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 10.2960 - accuracy: 0.1099 - val_loss: 11.5290 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 244/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.2965 - accuracy: 0.1087 - val_loss: 11.5326 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 245/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.2825 - accuracy: 0.1087 - val_loss: 11.5299 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 246/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.2011 - accuracy: 0.1066 - val_loss: 11.5300 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 247/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.2912 - accuracy: 0.1061 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 248/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.5075 - accuracy: 0.1085 - val_loss: 11.5294 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 249/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.4316 - accuracy: 0.1116 - val_loss: 11.5296 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 250/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3173 - accuracy: 0.1134 - val_loss: 11.5303 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 251/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.4287 - accuracy: 0.1117 - val_loss: 11.5295 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 252/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.4482 - accuracy: 0.1094 - val_loss: 11.5308 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 253/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.4368 - accuracy: 0.1057 - val_loss: 11.5293 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 254/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 10.5183 - accuracy: 0.1025 - val_loss: 11.5301 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 255/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.4626 - accuracy: 0.1031 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 256/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.5291 - accuracy: 0.1055 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 257/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.6443 - accuracy: 0.1031 - val_loss: 11.5293 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 258/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.2900 - accuracy: 0.1117 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 259/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.2998 - accuracy: 0.1151 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 260/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.4920 - accuracy: 0.1119 - val_loss: 11.5296 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 261/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3585 - accuracy: 0.1101 - val_loss: 11.5311 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 262/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3725 - accuracy: 0.1129 - val_loss: 11.5307 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 263/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.5490 - accuracy: 0.1134 - val_loss: 11.5290 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 264/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.4429 - accuracy: 0.1052 - val_loss: 11.5297 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 265/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.2518 - accuracy: 0.1111 - val_loss: 11.5295 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 266/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.4840 - accuracy: 0.1114 - val_loss: 11.5294 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 267/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.2650 - accuracy: 0.1069 - val_loss: 11.5297 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 268/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.5695 - accuracy: 0.1049 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 269/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3897 - accuracy: 0.1135 - val_loss: 11.5322 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 270/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3318 - accuracy: 0.1039 - val_loss: 11.5310 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 271/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3605 - accuracy: 0.1103 - val_loss: 11.5292 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 272/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 10.3197 - accuracy: 0.1068 - val_loss: 11.5305 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 273/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.4063 - accuracy: 0.1011 - val_loss: 11.5299 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 274/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.3593 - accuracy: 0.1089 - val_loss: 11.5289 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 275/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.2557 - accuracy: 0.1130 - val_loss: 11.5297 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 276/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.4047 - accuracy: 0.1054 - val_loss: 11.5298 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 277/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.4916 - accuracy: 0.1062 - val_loss: 11.5298 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 278/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.2831 - accuracy: 0.1108 - val_loss: 11.5297 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 279/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3583 - accuracy: 0.1091 - val_loss: 11.5299 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 280/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3050 - accuracy: 0.1054 - val_loss: 11.5288 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 281/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3868 - accuracy: 0.1120 - val_loss: 11.5302 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 282/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3715 - accuracy: 0.1055 - val_loss: 11.5302 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 283/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.3923 - accuracy: 0.1073 - val_loss: 11.5319 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 284/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.5859 - accuracy: 0.0984 - val_loss: 11.5302 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 285/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3609 - accuracy: 0.1099 - val_loss: 11.5298 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 286/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.4592 - accuracy: 0.1122 - val_loss: 11.5301 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 287/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 10.3489 - accuracy: 0.1117 - val_loss: 11.5302 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 288/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3857 - accuracy: 0.1020 - val_loss: 11.5291 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 289/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3647 - accuracy: 0.1143 - val_loss: 11.5307 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 290/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.4157 - accuracy: 0.1059 - val_loss: 11.5303 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 291/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3564 - accuracy: 0.1125 - val_loss: 11.5310 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 292/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.2731 - accuracy: 0.1150 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 293/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.5120 - accuracy: 0.1048 - val_loss: 11.5303 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 294/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.4883 - accuracy: 0.1095 - val_loss: 11.5351 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 295/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3998 - accuracy: 0.1062 - val_loss: 11.5292 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 296/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 10.3457 - accuracy: 0.1053 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 297/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.2182 - accuracy: 0.1156 - val_loss: 11.5295 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 298/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3862 - accuracy: 0.1063 - val_loss: 11.5297 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 299/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3852 - accuracy: 0.1173 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 300/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.2794 - accuracy: 0.1070 - val_loss: 11.5306 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 301/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.2784 - accuracy: 0.1054 - val_loss: 11.5306 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 302/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3212 - accuracy: 0.1125 - val_loss: 11.5288 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 303/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.2972 - accuracy: 0.1077 - val_loss: 11.5297 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 304/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.5539 - accuracy: 0.1074 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 305/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3546 - accuracy: 0.1121 - val_loss: 11.5293 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 306/500\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 10.4145 - accuracy: 0.1100 - val_loss: 11.5300 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 307/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.2989 - accuracy: 0.1052 - val_loss: 11.5300 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 308/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.0798 - accuracy: 0.1140 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 309/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.3290 - accuracy: 0.1108 - val_loss: 11.5287 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 310/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.2434 - accuracy: 0.0988 - val_loss: 11.5341 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 311/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.3373 - accuracy: 0.1164 - val_loss: 11.5326 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 312/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3796 - accuracy: 0.1159 - val_loss: 11.5312 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 313/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.3825 - accuracy: 0.1087 - val_loss: 11.5307 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 314/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3861 - accuracy: 0.1081 - val_loss: 11.5320 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 315/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.3603 - accuracy: 0.1080 - val_loss: 11.5305 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 316/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.2417 - accuracy: 0.1147 - val_loss: 11.5303 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 317/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.5076 - accuracy: 0.1172 - val_loss: 11.5297 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 318/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3111 - accuracy: 0.1025 - val_loss: 11.5331 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 319/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.4377 - accuracy: 0.1068 - val_loss: 11.5336 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 320/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3723 - accuracy: 0.1166 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 321/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.5300 - accuracy: 0.1067 - val_loss: 11.5299 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 322/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.5062 - accuracy: 0.1063 - val_loss: 11.5301 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 323/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3942 - accuracy: 0.1074 - val_loss: 11.5288 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 324/500\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 10.4197 - accuracy: 0.1104 - val_loss: 11.5291 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 325/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.3274 - accuracy: 0.1082 - val_loss: 11.5301 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 326/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.4612 - accuracy: 0.1101 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 327/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.5086 - accuracy: 0.1022 - val_loss: 11.5294 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 328/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 10.5144 - accuracy: 0.1032 - val_loss: 11.5294 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 329/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3800 - accuracy: 0.1129 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 330/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3192 - accuracy: 0.1093 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 331/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3963 - accuracy: 0.1154 - val_loss: 11.5289 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 332/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.4437 - accuracy: 0.1053 - val_loss: 11.5316 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 333/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.2971 - accuracy: 0.1119 - val_loss: 11.5305 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 334/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.2042 - accuracy: 0.1112 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 335/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.2754 - accuracy: 0.1174 - val_loss: 11.5298 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 336/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3238 - accuracy: 0.1186 - val_loss: 11.5289 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 337/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.4317 - accuracy: 0.1118 - val_loss: 11.5299 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 338/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.2025 - accuracy: 0.1014 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 339/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 10.4275 - accuracy: 0.1128 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 340/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.2338 - accuracy: 0.1100 - val_loss: 11.5311 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 341/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3288 - accuracy: 0.1124 - val_loss: 11.5302 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 342/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.2802 - accuracy: 0.1122 - val_loss: 11.5299 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 343/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.2445 - accuracy: 0.1035 - val_loss: 11.5305 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 344/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.2912 - accuracy: 0.1106 - val_loss: 11.5311 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 345/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.5502 - accuracy: 0.1084 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 346/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.2663 - accuracy: 0.1088 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 347/500\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 10.4874 - accuracy: 0.0990 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 348/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3641 - accuracy: 0.1091 - val_loss: 11.5297 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 349/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.2569 - accuracy: 0.1166 - val_loss: 11.5299 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 350/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3146 - accuracy: 0.1078 - val_loss: 11.5301 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 351/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.3313 - accuracy: 0.1149 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 352/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.4651 - accuracy: 0.1024 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 353/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.4238 - accuracy: 0.1153 - val_loss: 11.5346 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 354/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.4922 - accuracy: 0.1087 - val_loss: 11.5291 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 355/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.2455 - accuracy: 0.1154 - val_loss: 11.5297 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 356/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.2428 - accuracy: 0.1142 - val_loss: 11.5288 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 357/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.3815 - accuracy: 0.1138 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 358/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.5503 - accuracy: 0.1041 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 359/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.2084 - accuracy: 0.1116 - val_loss: 11.5294 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 360/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.4228 - accuracy: 0.1116 - val_loss: 11.5294 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 361/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.2967 - accuracy: 0.1143 - val_loss: 11.5289 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 362/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3077 - accuracy: 0.1096 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 363/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.4116 - accuracy: 0.1076 - val_loss: 11.5290 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 364/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.3826 - accuracy: 0.1077 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 365/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3163 - accuracy: 0.1204 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 366/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3290 - accuracy: 0.1141 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 367/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.3009 - accuracy: 0.1111 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 368/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3165 - accuracy: 0.1011 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 369/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.2697 - accuracy: 0.1095 - val_loss: 11.5298 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 370/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.2376 - accuracy: 0.1186 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 371/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.3693 - accuracy: 0.1038 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 372/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.2293 - accuracy: 0.1145 - val_loss: 11.5290 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 373/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.3721 - accuracy: 0.1140 - val_loss: 11.5303 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 374/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.4234 - accuracy: 0.1154 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 375/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.2623 - accuracy: 0.1086 - val_loss: 11.5294 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 376/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.5999 - accuracy: 0.1023 - val_loss: 11.5295 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 377/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.3997 - accuracy: 0.1099 - val_loss: 11.5287 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 378/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3729 - accuracy: 0.1128 - val_loss: 11.5290 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 379/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.4271 - accuracy: 0.1071 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 380/500\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 10.2052 - accuracy: 0.1087 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 381/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3894 - accuracy: 0.1020 - val_loss: 11.5309 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 382/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.5326 - accuracy: 0.0981 - val_loss: 11.5377 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 383/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.1866 - accuracy: 0.1134 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 384/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.5241 - accuracy: 0.1101 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 385/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.5587 - accuracy: 0.1070 - val_loss: 11.5298 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 386/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.1658 - accuracy: 0.1055 - val_loss: 11.5288 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 387/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3169 - accuracy: 0.1009 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 388/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.2808 - accuracy: 0.1070 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 389/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.3799 - accuracy: 0.1049 - val_loss: 11.5288 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 390/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.3758 - accuracy: 0.1028 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 391/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.4155 - accuracy: 0.1096 - val_loss: 11.5299 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 392/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.2523 - accuracy: 0.1202 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 393/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.5278 - accuracy: 0.1067 - val_loss: 11.5289 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 394/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3515 - accuracy: 0.1082 - val_loss: 11.5309 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 395/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.5198 - accuracy: 0.0992 - val_loss: 11.5322 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 396/500\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 10.3059 - accuracy: 0.1065 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 397/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.4344 - accuracy: 0.1082 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 398/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.3460 - accuracy: 0.1077 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 399/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.5235 - accuracy: 0.1103 - val_loss: 11.5293 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 400/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.4395 - accuracy: 0.1064 - val_loss: 11.5304 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 401/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.2781 - accuracy: 0.1115 - val_loss: 11.5287 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 402/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.3282 - accuracy: 0.1043 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 403/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.4161 - accuracy: 0.1097 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 404/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.2713 - accuracy: 0.1095 - val_loss: 11.5293 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 405/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.3075 - accuracy: 0.1098 - val_loss: 11.5301 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 406/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.3319 - accuracy: 0.1114 - val_loss: 11.5294 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 407/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.3969 - accuracy: 0.1158 - val_loss: 11.5292 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 408/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3730 - accuracy: 0.1069 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 409/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.5257 - accuracy: 0.1078 - val_loss: 11.5291 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 410/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.4498 - accuracy: 0.1080 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 411/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.1870 - accuracy: 0.1102 - val_loss: 11.5298 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 412/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3130 - accuracy: 0.1079 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 413/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.4682 - accuracy: 0.0990 - val_loss: 11.5290 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 414/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.2448 - accuracy: 0.1110 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 415/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.5494 - accuracy: 0.1006 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 416/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.4043 - accuracy: 0.1110 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 417/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.3764 - accuracy: 0.1024 - val_loss: 11.5294 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 418/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3709 - accuracy: 0.1068 - val_loss: 11.5290 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 419/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.3749 - accuracy: 0.1155 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 420/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3734 - accuracy: 0.1007 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 421/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.2110 - accuracy: 0.1095 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 422/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.2276 - accuracy: 0.1086 - val_loss: 11.5288 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 423/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 10.6116 - accuracy: 0.0970 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 424/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.2170 - accuracy: 0.1040 - val_loss: 11.5290 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 425/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.2113 - accuracy: 0.1075 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 426/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.1786 - accuracy: 0.1089 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 427/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.2232 - accuracy: 0.1060 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 428/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.4307 - accuracy: 0.1078 - val_loss: 11.5288 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 429/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.5642 - accuracy: 0.1012 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 430/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.2837 - accuracy: 0.1073 - val_loss: 11.5319 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 431/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.2062 - accuracy: 0.1134 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 432/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.2530 - accuracy: 0.1111 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 433/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.5402 - accuracy: 0.1052 - val_loss: 11.5307 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 434/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.2250 - accuracy: 0.1007 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 435/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.4590 - accuracy: 0.1097 - val_loss: 11.5301 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 436/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3639 - accuracy: 0.1093 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 437/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.2003 - accuracy: 0.1052 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 438/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.5059 - accuracy: 0.1078 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 439/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.4925 - accuracy: 0.1123 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 440/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.4647 - accuracy: 0.1060 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 441/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.4341 - accuracy: 0.1056 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 442/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.3618 - accuracy: 0.1021 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 443/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 10.4050 - accuracy: 0.1006 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 444/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.5955 - accuracy: 0.1074 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 445/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.3164 - accuracy: 0.1094 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 446/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.2939 - accuracy: 0.1077 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 447/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.2915 - accuracy: 0.1097 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 448/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.3738 - accuracy: 0.1106 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 449/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.2221 - accuracy: 0.1096 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 450/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.4496 - accuracy: 0.1128 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 451/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.5076 - accuracy: 0.0964 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 452/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.5690 - accuracy: 0.1041 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 453/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3561 - accuracy: 0.1089 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 454/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3923 - accuracy: 0.1111 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 455/500\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 10.4042 - accuracy: 0.1036 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 456/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.3690 - accuracy: 0.1049 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 457/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3204 - accuracy: 0.1181 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 458/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.3539 - accuracy: 0.1041 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 459/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.4160 - accuracy: 0.1078 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 460/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.2928 - accuracy: 0.1133 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 461/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.2846 - accuracy: 0.1150 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 462/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3631 - accuracy: 0.1047 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 463/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3371 - accuracy: 0.1044 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 464/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.4142 - accuracy: 0.1076 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 465/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.3238 - accuracy: 0.1050 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 466/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.1274 - accuracy: 0.1107 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 467/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.3215 - accuracy: 0.1091 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 468/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.1634 - accuracy: 0.1132 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 469/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.4035 - accuracy: 0.1057 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 470/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.4542 - accuracy: 0.1142 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 471/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.2939 - accuracy: 0.1118 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 472/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.3166 - accuracy: 0.1094 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 473/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.2932 - accuracy: 0.1188 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 474/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3316 - accuracy: 0.1068 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 475/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3935 - accuracy: 0.1086 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 476/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.4594 - accuracy: 0.1102 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 477/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.3272 - accuracy: 0.1058 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 478/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.2063 - accuracy: 0.1064 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 479/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.4186 - accuracy: 0.1088 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 480/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.5217 - accuracy: 0.1102 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 481/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.3927 - accuracy: 0.1081 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 482/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.5183 - accuracy: 0.1103 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 483/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.3049 - accuracy: 0.0977 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 484/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.2890 - accuracy: 0.1167 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 485/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 10.4545 - accuracy: 0.1121 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 486/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.1659 - accuracy: 0.1164 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 487/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.4676 - accuracy: 0.1085 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 488/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.2733 - accuracy: 0.1152 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 489/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3104 - accuracy: 0.1077 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 490/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.4320 - accuracy: 0.1125 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 491/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3672 - accuracy: 0.1047 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 492/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.5082 - accuracy: 0.1121 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 493/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.4177 - accuracy: 0.1087 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 494/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3455 - accuracy: 0.1132 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 495/500\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 10.1669 - accuracy: 0.1160 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 496/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.4119 - accuracy: 0.1063 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 497/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.2294 - accuracy: 0.1076 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 498/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 10.4890 - accuracy: 0.1003 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 499/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3006 - accuracy: 0.1060 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n",
      "9.999999747378752e-05\n",
      "Epoch 500/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 10.3217 - accuracy: 0.1100 - val_loss: 11.5286 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1728x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#compile and train\n",
    "def MultiStepLRSchedule(epoch, lr):\n",
    "    milestones = []\n",
    "    gamma = 5e-4\n",
    "    print(lr)\n",
    "    for i in range(1,len(milestones)+1):\n",
    "        if(epoch == milestones[-i]):\n",
    "            return lr*gamma\n",
    "    return lr\n",
    "\n",
    "callbacks = [tf.keras.callbacks.LearningRateScheduler(MultiStepLRSchedule)]\n",
    "scheduler = tf.keras.optimizers.schedules.LearningRateSchedule\n",
    "#opt = tf.keras.optimizers.SGD(learning_rate = 1e-4, momentum=0.9)\n",
    "opt = tf.keras.optimizers.Adam(learning_rate = 1e-4)\n",
    "model.compile(optimizer = opt , loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "hist = model.fit(train_dataset,\n",
    "                 callbacks=callbacks,\n",
    "                 validation_data = test_dataset,\n",
    "                 epochs=500\n",
    "                )\n",
    "\n",
    "#plot as graph\n",
    "plt.figure(figsize=(24,6))\n",
    "loss_ax = plt.subplot(121)\n",
    "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "loss_ax.legend(loc='upper left')\n",
    "\n",
    "acc_ax = plt.subplot(122)\n",
    "acc_ax.plot(hist.history['accuracy'], 'b', label='train acc')\n",
    "acc_ax.plot(hist.history['val_accuracy'], 'g', label='val acc')\n",
    "acc_ax.set_ylabel('accuracy')\n",
    "acc_ax.legend(loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#evaluate\n",
    "test_loss , test_acc = model.evaluate(test_dataset, verbose=2)\n",
    "print(test_acc)\n",
    "pred = model.predict(test_dataset.take(1))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# \n",
    "model.save('HARCNN.pb')\n",
    "# \n",
    "export_path = ('/jjw')\n",
    "model.save(export_path, save_format='tf')\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model('HARCNN.pb')\n",
    "tfmodel=converter.convert()\n",
    "open('har_cnn.tflite','wb').write(tfmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ 2.28298711e+01  2.90968945e+00 -2.87611611e+01 ... -2.40608936e+01\n",
      "     2.69705830e+01 -7.27422363e+01]\n",
      "   [ 2.61872051e+01 -5.28220547e+01  1.89129814e+01 ...  8.95289062e-01\n",
      "     3.89450742e+01 -1.48841807e+01]\n",
      "   [-3.35733398e-01 -6.33417012e+01 -4.75622314e+01 ... -6.69228574e+01\n",
      "     1.23326068e+02 -9.90413525e+01]\n",
      "   ...\n",
      "   [-1.21093750e+00 -4.42968750e+00 -5.30859375e+00 ...  4.30468750e+00\n",
      "    -4.80859375e+00  4.26953125e+00]\n",
      "   [-5.31250000e-01 -1.28671875e+01 -8.38281250e+00 ... -7.61718750e-01\n",
      "     2.80078125e+00 -6.31640625e+00]\n",
      "   [ 2.61872051e+01 -5.28220547e+01  1.89129814e+01 ...  8.95289062e-01\n",
      "     3.89450742e+01 -1.48841807e+01]]\n",
      "\n",
      "  [[ 1.86891592e+01 -3.29018730e+01  5.20386768e+01 ... -1.15268467e+01\n",
      "     1.94725371e+01 -6.00962783e+01]\n",
      "   [ 3.91688965e+01  2.29417822e+01 -2.23822266e-01 ...  4.25262305e+00\n",
      "     4.51001865e+01  1.74581367e+01]\n",
      "   [-8.50524609e+00 -7.67710371e+01  1.16499489e+02 ... -6.40131680e+01\n",
      "     8.93050840e+01 -9.84817969e+01]\n",
      "   ...\n",
      "   [-7.89062500e-01  1.69921875e+00 -5.19531250e+00 ...  4.61718750e+00\n",
      "    -4.30468750e+00  4.00000000e+00]\n",
      "   [-8.39843750e-01  8.47656250e-01 -7.46093750e+00 ... -2.78906250e+00\n",
      "     3.33984375e+00 -3.25000000e+00]\n",
      "   [ 3.91688965e+01  2.29417822e+01 -2.23822266e-01 ...  4.25262305e+00\n",
      "     4.51001865e+01  1.74581367e+01]]\n",
      "\n",
      "  [[ 1.58913809e+01 -3.23423174e+01  4.73384092e+01 ... -8.72906836e+00\n",
      "     1.81296035e+01 -4.19666748e+01]\n",
      "   [ 4.72264980e+01  1.92487148e+01 -1.90248926e+00 ...  1.41008027e+01\n",
      "     4.16309414e+01  4.12952080e+01]\n",
      "   [-1.39888916e+01 -7.26303252e+01  1.08329977e+02 ... -4.51001865e+01\n",
      "     7.39732588e+01 -8.59477500e+01]\n",
      "   ...\n",
      "   [-6.75781250e-01  1.62890625e+00 -4.92578125e+00 ...  2.31640625e+00\n",
      "    -3.77343750e+00  3.15625000e+00]\n",
      "   [-6.48437500e-01  1.38281250e+00 -4.48046875e+00 ... -2.59375000e+00\n",
      "     2.80078125e+00 -8.75000000e-01]\n",
      "   [ 4.72264980e+01  1.92487148e+01 -1.90248926e+00 ...  1.41008027e+01\n",
      "     4.16309414e+01  4.12952080e+01]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-4.87932539e+01  8.35976162e+01 -1.37650693e+01 ...  6.91610801e+01\n",
      "    -6.88253467e+01  2.58514717e+01]\n",
      "   [ 2.42847158e+01 -1.42127139e+01 -2.17890976e+02 ...  2.27179600e+01\n",
      "     2.79777832e+00 -5.68508555e+01]\n",
      "   [-8.38214385e+01  1.28138247e+02 -7.80020596e+01 ...  1.60592476e+02\n",
      "    -1.34517182e+02  1.75700479e+01]\n",
      "   ...\n",
      "   [ 3.11718750e+00 -2.14843750e-01 -6.64843750e+00 ... -6.18750000e+00\n",
      "     2.27734375e+00  5.41796875e+00]\n",
      "   [-4.70312500e+00 -1.38203125e+01 -1.93398438e+01 ... -1.48828125e+00\n",
      "    -1.23320312e+01 -2.59648438e+01]\n",
      "   [ 2.42847158e+01 -1.42127139e+01 -2.17890976e+02 ...  2.27179600e+01\n",
      "     2.79777832e+00 -5.68508555e+01]]\n",
      "\n",
      "  [[-4.09594746e+01  8.93050840e+01 -2.41728047e+01 ...  5.14791211e+01\n",
      "    -7.57638369e+01  2.02559150e+01]\n",
      "   [ 2.58514717e+01  1.05196465e+01 -1.71224033e+01 ...  2.19345820e+01\n",
      "    -1.62271143e+01  8.91931729e+01]\n",
      "   [-7.82258818e+01  1.36195849e+02 -1.78945901e+02 ...  1.59473364e+02\n",
      "    -1.07434687e+02  1.02734420e+02]\n",
      "   ...\n",
      "   [ 2.35156250e+00 -1.82421875e+00 -2.55078125e+00 ... -5.99609375e+00\n",
      "     3.46484375e+00  2.77734375e+00]\n",
      "   [-2.40625000e+00 -1.38984375e+01 -2.21757812e+01 ...  3.51562500e-01\n",
      "    -9.37500000e+00 -2.41289062e+01]\n",
      "   [ 2.58514717e+01  1.05196465e+01 -1.71224033e+01 ...  2.19345820e+01\n",
      "    -1.62271143e+01  8.91931729e+01]]\n",
      "\n",
      "  [[-3.80497852e+01  8.84097949e+01 -4.87932539e+01 ...  3.86093408e+01\n",
      "    -7.57638369e+01  3.77140518e+01]\n",
      "   [ 2.76420498e+01  2.29417822e+01  6.61394795e+01 ...  2.80896943e+01\n",
      "    -1.95844482e+01  7.47566367e+01]\n",
      "   [-7.86735264e+01  1.39329360e+02 -1.80288835e+02 ...  1.45036828e+02\n",
      "    -1.00384286e+02  1.51751496e+02]\n",
      "   ...\n",
      "   [ 2.31640625e+00 -3.05078125e+00  6.28906250e-01 ... -5.42578125e+00\n",
      "     3.77343750e+00  1.24609375e+00]\n",
      "   [-1.71875000e+00 -1.35585938e+01 -2.43632812e+01 ...  1.53906250e+00\n",
      "    -8.34765625e+00 -1.82656250e+01]\n",
      "   [ 2.76420498e+01  2.29417822e+01  6.61394795e+01 ...  2.80896943e+01\n",
      "    -1.95844482e+01  7.47566367e+01]]]]\n",
      "[[[[ 1.95620660e+02  1.35412471e+01  3.42448066e+01 ...  1.85212925e+02\n",
      "     2.32663245e+02  1.29369270e+02]\n",
      "   [-1.20864023e+01 -2.86492500e+01  1.92487148e+01 ...  1.07770421e+02\n",
      "     8.95289063e+00 -1.99201816e+01]\n",
      "   [ 4.02880078e+01  7.94569043e+00  8.23665937e+01 ... -9.73626855e+00\n",
      "     7.55400146e+01  1.00720020e+02]\n",
      "   ...\n",
      "   [ 2.40234375e+00  4.49218750e-01 -5.25781250e+00 ...  5.27343750e+00\n",
      "    -6.75390625e+00  4.01171875e+00]\n",
      "   [-2.22265625e+00  4.57031250e-01  1.53125000e+00 ... -1.24062500e+01\n",
      "    -4.51210938e+01 -5.55390625e+01]\n",
      "   [-1.20864023e+01 -2.86492500e+01  1.92487148e+01 ...  1.07770421e+02\n",
      "     8.95289063e+00 -1.99201816e+01]]\n",
      "\n",
      "  [[ 1.78498257e+02 -8.50524609e+00  7.58757480e+01 ...  1.75588567e+02\n",
      "     2.06364129e+02  1.20192557e+02]\n",
      "   [-1.55556475e+01 -2.04797373e+01  1.06315576e+01 ...  1.03965442e+02\n",
      "    -1.90248926e+00 -2.60752939e+01]\n",
      "   [ 1.91368037e+01  3.46924512e+00  8.89693506e+01 ... -4.47644531e+00\n",
      "     1.10792021e+01  1.14932733e+02]\n",
      "   ...\n",
      "   [ 1.79296875e+00 -8.51562500e-01 -2.11718750e+00 ...  5.08203125e+00\n",
      "     1.63671875e+00  2.38906250e+01]\n",
      "   [-9.57031250e-01  1.03125000e+00 -1.45703125e+00 ... -9.76953125e+00\n",
      "    -2.11054688e+01 -7.23593750e+01]\n",
      "   [-1.55556475e+01 -2.04797373e+01  1.06315576e+01 ...  1.03965442e+02\n",
      "    -1.90248926e+00 -2.60752939e+01]]\n",
      "\n",
      "  [[ 1.26011936e+02 -4.11832969e+01  9.91532637e+01 ...  1.78162523e+02\n",
      "     8.31499717e+01  1.36419671e+02]\n",
      "   [-2.20464932e+01 -4.58835645e+00  8.72906836e+00 ...  1.01055753e+02\n",
      "     3.35733398e-01 -3.24542285e+00]\n",
      "   [-1.46603584e+01 -1.90248926e+00  9.09837510e+01 ...  2.08154707e+01\n",
      "    -1.14820822e+02  2.71944053e+01]\n",
      "   ...\n",
      "   [ 1.83593750e-01 -2.84375000e+00 -4.29687500e-01 ...  5.27343750e+00\n",
      "     5.62109375e+00  1.28203125e+01]\n",
      "   [ 7.26562500e-01  1.49218750e+00 -3.44531250e+00 ... -7.46093750e+00\n",
      "    -1.55859375e+01 -2.68437500e+01]\n",
      "   [-2.20464932e+01 -4.58835645e+00  8.72906836e+00 ...  1.01055753e+02\n",
      "     3.35733398e-01 -3.24542285e+00]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 3.15589395e+01  6.19987676e+01  1.83758080e+02 ...  5.78580557e+01\n",
      "     6.04320117e+01  5.12552988e+01]\n",
      "   [ 4.58835645e+00 -1.41008027e+01  1.17059045e+02 ... -7.38613477e+00\n",
      "    -1.19744912e+01  2.35013379e+01]\n",
      "   [ 2.03678262e+01  4.71145869e+01  2.73063164e+01 ...  4.77860537e+01\n",
      "     3.83855186e+01  3.01040947e+01]\n",
      "   ...\n",
      "   [ 3.90234375e+00  2.57812500e-01  5.35156250e+00 ...  6.30859375e+00\n",
      "     1.06250000e+00  2.47656250e+00]\n",
      "   [-1.45703125e+00 -9.53125000e-01 -1.36015625e+01 ... -3.00664062e+01\n",
      "    -2.83203125e+00 -8.00781250e-01]\n",
      "   [ 4.58835645e+00 -1.41008027e+01  1.17059045e+02 ... -7.38613477e+00\n",
      "    -1.19744912e+01  2.35013379e+01]]\n",
      "\n",
      "  [[-3.80497852e+00  8.89693506e+01  1.73909900e+02 ... -6.65871240e+01\n",
      "     7.38613477e+01  2.59633828e+01]\n",
      "   [ 3.91688965e+00 -1.64509365e+01  1.07546599e+02 ... -1.72343145e+01\n",
      "    -2.05916484e+01  2.66348496e+01]\n",
      "   [ 1.83534258e+01  3.22304063e+01  1.16387578e+01 ... -9.81460635e+01\n",
      "     3.65949404e+01  3.21184951e+01]\n",
      "   ...\n",
      "   [ 2.36718750e+00  1.17968750e+00  4.58593750e+00 ...  4.50781250e+00\n",
      "     2.55859375e+00  2.57812500e-01]\n",
      "   [-7.81250000e-02  1.13281250e-01 -1.05703125e+01 ... -1.76210938e+01\n",
      "    -1.60546875e+00  7.30468750e-01]\n",
      "   [ 3.91688965e+00 -1.64509365e+01  1.07546599e+02 ... -1.72343145e+01\n",
      "    -2.05916484e+01  2.66348496e+01]]\n",
      "\n",
      "  [[-3.88331631e+01  9.45649072e+01  1.73014611e+02 ... -8.14713047e+01\n",
      "     4.76741426e+01  1.86891592e+01]\n",
      "   [ 6.82657910e+00 -1.24221357e+01  9.74745967e+01 ... -2.46204492e+00\n",
      "    -2.43966270e+01  2.94326279e+01]\n",
      "   [ 8.50524609e+00  5.25982324e+00  5.59555664e+00 ... -2.04125906e+02\n",
      "     2.17107598e+01  2.99921836e+01]\n",
      "   ...\n",
      "   [ 5.66406250e-01 -7.38281250e-01  4.58593750e+00 ...  1.75000000e+00\n",
      "     2.39843750e+00 -3.94531250e-01]\n",
      "   [ 1.99218750e+00  1.68359375e+00 -8.58203125e+00 ... -1.41328125e+01\n",
      "    -1.03125000e+00  1.42187500e+00]\n",
      "   [ 6.82657910e+00 -1.24221357e+01  9.74745967e+01 ... -2.46204492e+00\n",
      "    -2.43966270e+01  2.94326279e+01]]]]\n",
      "[[[[ 3.84974297e+01 -2.73063164e+01 -6.71466797e-01 ...  8.50524609e+00\n",
      "     1.19744912e+01  4.14071191e+00]\n",
      "   [-1.67866699e+01  1.00720020e+01 -3.13351172e+00 ... -1.34293359e+00\n",
      "    -1.53318252e+01  2.12631152e+00]\n",
      "   [-1.34293359e+01 -5.25982324e+00 -1.79057812e+00 ... -5.81937891e+00\n",
      "     3.91688965e+00 -1.10792021e+01]\n",
      "   ...\n",
      "   [-1.09375000e-01  1.07812500e+00 -6.44531250e-01 ...  3.47656250e-01\n",
      "    -3.78906250e-01 -2.61718750e-01]\n",
      "   [-1.01132812e+01 -1.22890625e+01 -1.02617188e+01 ... -1.02617188e+01\n",
      "    -7.77734375e+00 -7.04687500e+00]\n",
      "   [-1.67866699e+01  1.00720020e+01 -3.13351172e+00 ... -1.34293359e+00\n",
      "    -1.53318252e+01  2.12631152e+00]]\n",
      "\n",
      "  [[ 3.96165410e+01 -3.83855186e+01 -2.12631152e+00 ...  6.37893457e+00\n",
      "     8.84097949e+00  3.46924512e+00]\n",
      "   [-1.51080029e+01  3.13351172e+00 -4.25262305e+00 ... -1.11911133e+00\n",
      "    -1.73462256e+01 -8.95289062e-01]\n",
      "   [-5.93129004e+00 -4.36453418e+00 -2.90968945e+00 ... -6.37893457e+00\n",
      "     5.48364551e+00 -1.17506689e+01]\n",
      "   ...\n",
      "   [-1.48437500e-01  1.95703125e+00 -5.70312500e-01 ...  2.34375000e-01\n",
      "    -4.17968750e-01 -5.31250000e-01]\n",
      "   [-9.68750000e+00 -1.39804688e+01 -1.03750000e+01 ... -1.04531250e+01\n",
      "    -8.19921875e+00 -6.77343750e+00]\n",
      "   [-1.51080029e+01  3.13351172e+00 -4.25262305e+00 ... -1.11911133e+00\n",
      "    -1.73462256e+01 -8.95289062e-01]]\n",
      "\n",
      "  [[ 3.98403633e+01 -5.49483662e+01 -3.58115625e+00 ...  5.25982324e+00\n",
      "     7.72186816e+00  3.13351172e+00]\n",
      "   [-1.25340469e+01 -6.60275684e+00 -5.59555664e+00 ... -1.56675586e+00\n",
      "    -1.79057813e+01 -5.59555664e+00]\n",
      "   [-2.79777832e+00 -2.01440039e+00 -3.46924512e+00 ... -6.04320117e+00\n",
      "     6.37893457e+00 -1.26459580e+01]\n",
      "   ...\n",
      "   [-1.48437500e-01  3.14453125e+00  3.90625000e-03 ...  2.34375000e-01\n",
      "    -3.78906250e-01 -6.09375000e-01]\n",
      "   [-9.49609375e+00 -1.73867188e+01 -1.02304688e+01 ... -1.04531250e+01\n",
      "    -8.34375000e+00 -6.85156250e+00]\n",
      "   [-1.25340469e+01 -6.60275684e+00 -5.59555664e+00 ... -1.56675586e+00\n",
      "    -1.79057813e+01 -5.59555664e+00]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-1.71224033e+01  3.91688965e+00  5.25982324e+00 ...  1.62271143e+01\n",
      "     5.70746777e+00  3.80497852e+00]\n",
      "   [ 1.00720020e+01 -6.15511230e+00 -6.93849023e+00 ... -1.16387578e+01\n",
      "     4.70026758e+00 -4.92408984e+00]\n",
      "   [-5.14791211e+00  1.34293359e+00 -5.93129004e+00 ... -6.71466797e+00\n",
      "    -1.14149355e+01  1.34293359e+00]\n",
      "   ...\n",
      "   [-1.91015625e+00 -5.70312500e-01 -4.92187500e-01 ... -7.22656250e-01\n",
      "     1.21093750e-01 -3.51562500e-02]\n",
      "   [-1.04960938e+01 -1.04960938e+01 -8.00000000e+00 ... -7.08203125e+00\n",
      "    -7.61718750e+00 -1.02304688e+01]\n",
      "   [ 1.00720020e+01 -6.15511230e+00 -6.93849023e+00 ... -1.16387578e+01\n",
      "     4.70026758e+00 -4.92408984e+00]]\n",
      "\n",
      "  [[-1.58913809e+01  2.35013379e+00  5.25982324e+00 ...  1.53318252e+01\n",
      "     5.48364551e+00  1.21983135e+01]\n",
      "   [ 1.27578691e+01 -4.02880078e+00 -8.05760156e+00 ... -1.19744912e+01\n",
      "     4.02880078e+00  1.19744912e+01]\n",
      "   [-5.93129004e+00  2.23822266e-01 -5.14791211e+00 ... -3.35733398e+00\n",
      "    -1.05196465e+01 -7.83377930e-01]\n",
      "   ...\n",
      "   [-1.14453125e+00 -4.53125000e-01 -5.70312500e-01 ... -4.92187500e-01\n",
      "     8.20312500e-02 -2.26562500e-01]\n",
      "   [-1.13359375e+01 -1.04179688e+01 -8.15625000e+00 ... -7.35546875e+00\n",
      "    -7.35156250e+00 -1.00703125e+01]\n",
      "   [ 1.27578691e+01 -4.02880078e+00 -8.05760156e+00 ... -1.19744912e+01\n",
      "     4.02880078e+00  1.19744912e+01]]\n",
      "\n",
      "  [[-2.30536934e+01  1.34293359e+00  7.27422363e+00 ...  1.39888916e+01\n",
      "     4.81217871e+00  1.47722695e+01]\n",
      "   [ 1.25340469e+01 -3.58115625e+00 -9.28862402e+00 ... -1.21983135e+01\n",
      "     3.35733398e+00  1.71224033e+01]\n",
      "   [-5.70746777e+00 -3.35733398e-01 -3.46924512e+00 ... -4.47644531e-01\n",
      "    -1.05196465e+01 -4.47644531e-01]\n",
      "   ...\n",
      "   [-2.26562500e-01 -6.09375000e-01 -6.44531250e-01 ... -3.78906250e-01\n",
      "    -1.09375000e-01 -2.26562500e-01]\n",
      "   [-1.14453125e+01 -1.01875000e+01 -8.42187500e+00 ... -7.61718750e+00\n",
      "    -7.12109375e+00 -1.01875000e+01]\n",
      "   [ 1.25340469e+01 -3.58115625e+00 -9.28862402e+00 ... -1.21983135e+01\n",
      "     3.35733398e+00  1.71224033e+01]]]]\n",
      "[[[[  1.34293359  -4.14071191   7.38613477 ... -23.61324902\n",
      "    -24.39662695  12.86978027]\n",
      "   [ 29.54453906  42.973875    11.07920215 ...  30.99938379\n",
      "     15.44373633  40.28800781]\n",
      "   [ -7.05040137   2.12631152 -10.96729102 ... -12.31022461\n",
      "     -9.40053516 -15.89138086]\n",
      "   ...\n",
      "   [  1.92578125   1.54296875   1.42578125 ...   1.2734375\n",
      "      0.5078125   -1.1796875 ]\n",
      "   [ -8.91015625  -6.83984375 -10.890625   ...  -9.59765625\n",
      "     -9.48046875  -7.3671875 ]\n",
      "   [ 29.54453906  42.973875    11.07920215 ...  30.99938379\n",
      "     15.44373633  40.28800781]]\n",
      "\n",
      "  [[  2.35013379  -4.02880078  14.6603584  ...  -6.37893457\n",
      "    -31.33511719   9.84817969]\n",
      "   [ 31.33511719  43.53343066  10.85537988 ...  38.27360742\n",
      "     17.90578125  19.02489258]\n",
      "   [ -6.4908457    1.90248926  -4.58835645 ... -15.44373633\n",
      "     -9.17671289  -0.11191113]\n",
      "   ...\n",
      "   [  1.6953125    1.7734375    1.80859375 ...   2.578125\n",
      "      0.62109375  -1.25390625]\n",
      "   [ -9.0546875   -6.7578125  -10.16796875 ... -10.015625\n",
      "     -9.67578125  -7.56640625]\n",
      "   [ 31.33511719  43.53343066  10.85537988 ...  38.27360742\n",
      "     17.90578125  19.02489258]]\n",
      "\n",
      "  [[  2.57395605  -2.68586719  15.55564746 ...  -1.79057812\n",
      "    -39.39271875   8.72906836]\n",
      "   [ 32.34231738  44.42871973  14.10080273 ...  35.1400957\n",
      "     19.36062598  17.23431445]\n",
      "   [ -5.81937891   0.3357334   -2.46204492 ... -16.67475879\n",
      "     -7.94569043   2.01440039]\n",
      "   ...\n",
      "   [  1.6171875    1.88671875   1.88671875 ...   2.15625\n",
      "      1.12109375  -1.21484375]\n",
      "   [ -9.0546875   -6.8359375   -9.25390625 ... -10.28515625\n",
      "    -10.01953125  -7.68359375]\n",
      "   [ 32.34231738  44.42871973  14.10080273 ...  35.1400957\n",
      "     19.36062598  17.23431445]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ -4.92408984   0.78337793 -15.55564746 ... -12.31022461\n",
      "      7.38613477   8.95289063]\n",
      "   [ 38.72125195  33.79716211   8.1695127  ...   8.50524609\n",
      "     40.17609668  55.17218848]\n",
      "   [  0.22382227 -11.86258008  -3.69306738 ...  -7.38613477\n",
      "    -12.98169141 -21.59884863]\n",
      "   ...\n",
      "   [  1.3125       2.578125     0.73828125 ...   0.546875\n",
      "     -0.375        0.203125  ]\n",
      "   [ -7.87109375  -9.9375      -9.44140625 ...  -9.5546875\n",
      "     -8.44921875  -9.2109375 ]\n",
      "   [ 38.72125195  33.79716211   8.1695127  ...   8.50524609\n",
      "     40.17609668  55.17218848]]\n",
      "\n",
      "  [[ -3.91688965   1.23102246 -19.69635938 ... -16.33902539\n",
      "      6.26702344   6.4908457 ]\n",
      "   [ 41.07138574  26.63484961  10.07200195 ...  10.18391309\n",
      "     36.70685156  54.05307715]\n",
      "   [  1.11911133 -13.3174248   -3.58115625 ...  -8.39333496\n",
      "    -11.75066895 -21.71075977]\n",
      "   ...\n",
      "   [  1.2734375    1.84765625   0.890625   ...   0.5859375\n",
      "     -0.3359375    0.125     ]\n",
      "   [ -7.3671875  -10.890625    -9.63671875 ...  -9.51953125\n",
      "     -8.75390625  -9.2109375 ]\n",
      "   [ 41.07138574  26.63484961  10.07200195 ...  10.18391309\n",
      "     36.70685156  54.05307715]]\n",
      "\n",
      "  [[ -4.02880078   2.35013379 -22.60604883 ... -19.58444824\n",
      "      6.37893457   4.81217871]\n",
      "   [ 42.86196387  18.68915918  11.19111328 ...  13.20551367\n",
      "     34.69245117  52.03867676]\n",
      "   [  1.67866699 -12.19831348  -3.69306738 ...  -9.17671289\n",
      "    -11.19111328 -20.36782617]\n",
      "   ...\n",
      "   [  1.2734375    1.16015625   0.96875    ...   0.5078125\n",
      "     -0.375        0.125     ]\n",
      "   [ -7.22265625 -11.625       -9.67578125 ...  -9.4765625\n",
      "     -8.90625     -8.9375    ]\n",
      "   [ 42.86196387  18.68915918  11.19111328 ...  13.20551367\n",
      "     34.69245117  52.03867676]]]]\n",
      "[[[[ 7.84497041e+01  2.77539609e+01 -1.62271143e+01 ...  1.80736479e+02\n",
      "    -4.65550312e+01  1.62830698e+02]\n",
      "   [ 5.59555664e+00 -1.85772480e+01  2.89849834e+01 ...  7.72186816e+01\n",
      "    -2.49561826e+01  4.14071191e+00]\n",
      "   [ 3.11112949e+01 -8.06879268e+01 -4.55478311e+01 ...  1.81296035e+01\n",
      "    -2.45085381e+02  8.55001055e+01]\n",
      "   ...\n",
      "   [ 8.98437500e-01  2.75585938e+01 -2.35937500e+00 ...  3.92578125e+00\n",
      "     2.16406250e+00 -3.31640625e+00]\n",
      "   [-2.16718750e+01 -7.68359375e+01  5.02343750e+00 ... -4.01171875e+00\n",
      "    -1.82968750e+01 -1.90273438e+01]\n",
      "   [ 5.59555664e+00 -1.85772480e+01  2.89849834e+01 ...  7.72186816e+01\n",
      "    -2.49561826e+01  4.14071191e+00]]\n",
      "\n",
      "  [[ 5.37173437e+01  4.72264980e+01 -1.60032920e+01 ...  1.72902700e+02\n",
      "    -4.16309414e+01  4.56597422e+01]\n",
      "   [ 3.13351172e+00 -2.07035596e+01  2.74182275e+01 ...  8.00164600e+01\n",
      "    -4.43168086e+01  4.71145869e+01]\n",
      "   [ 2.65229385e+01 -4.67788535e+01 -9.24385957e+01 ...  1.68985811e+01\n",
      "    -2.11847774e+02  4.81217871e+00]\n",
      "   ...\n",
      "   [ 1.28125000e+00  1.56445312e+01 -2.70312500e+00 ...  4.23046875e+00\n",
      "    -9.02343750e-01  1.55078125e+00]\n",
      "   [-2.16328125e+01 -4.38125000e+01  3.03515625e+00 ... -3.05468750e+00\n",
      "    -1.47734375e+01 -2.29023438e+01]\n",
      "   [ 3.13351172e+00 -2.07035596e+01  2.74182275e+01 ...  8.00164600e+01\n",
      "    -4.43168086e+01  4.71145869e+01]]\n",
      "\n",
      "  [[-1.52199141e+02  6.21106787e+01 -2.00320928e+01 ...  1.53877808e+02\n",
      "    -1.10456288e+02  3.09993838e+01]\n",
      "   [-2.52919160e+01 -1.91368037e+01  2.04797373e+01 ...  9.26624180e+01\n",
      "    -3.63711182e+01  3.79378740e+01]\n",
      "   [-1.70776389e+02  1.43246250e+01 -1.12694511e+02 ...  1.55556475e+01\n",
      "    -2.57395605e+00 -1.28809714e+02]\n",
      "   ...\n",
      "   [ 2.23828125e+00 -8.24218750e-01 -8.63281250e-01 ...  4.61328125e+00\n",
      "    -1.93750000e+00  4.57421875e+00]\n",
      "   [-1.88007812e+01 -1.63828125e+01 -1.98828125e+00 ... -1.40625000e+00\n",
      "    -1.23593750e+01 -1.93398438e+01]\n",
      "   [-2.52919160e+01 -1.91368037e+01  2.04797373e+01 ...  9.26624180e+01\n",
      "    -3.63711182e+01  3.79378740e+01]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-2.73063164e+01 -3.06636504e+01 -1.02286775e+02 ...  4.98004541e+01\n",
      "     5.30458770e+01 -2.42847158e+01]\n",
      "   [-2.78658721e+01  1.29816914e+01 -9.51244629e+00 ... -1.65628477e+01\n",
      "    -1.34293359e+01  2.46204492e+00]\n",
      "   [-1.28250158e+02  2.45085381e+01  5.64032109e+01 ...  6.86015244e+01\n",
      "    -1.48841807e+01 -9.49006406e+01]\n",
      "   ...\n",
      "   [-6.15234375e+00 -9.41406250e-01 -2.08984375e+00 ...  1.22734375e+01\n",
      "     3.92578125e+00 -9.98046875e+00]\n",
      "   [-4.55078125e+00  3.80078125e+00 -4.39062500e+00 ... -2.33242188e+01\n",
      "    -1.91015625e+01 -5.99023438e+01]\n",
      "   [-2.78658721e+01  1.29816914e+01 -9.51244629e+00 ... -1.65628477e+01\n",
      "    -1.34293359e+01  2.46204492e+00]]\n",
      "\n",
      "  [[-7.41970811e+01 -4.18547637e+01 -6.45727236e+01 ...  6.32297900e+01\n",
      "     8.46048164e+01 -4.07356523e+01]\n",
      "   [-2.38370713e+01  1.34293359e+01 -1.34293359e+01 ... -1.08553799e+01\n",
      "    -1.10792021e+01  1.67866699e+00]\n",
      "   [-1.41119938e+02  2.55157383e+01  5.34935215e+01 ...  5.65151221e+01\n",
      "     9.84817969e+00 -6.98325469e+01]\n",
      "   ...\n",
      "   [-3.12500000e+00 -1.01562500e+00 -1.59375000e+00 ...  9.55468750e+00\n",
      "    -5.85937500e-02 -8.83203125e+00]\n",
      "   [-1.04882812e+01  5.68359375e+00 -3.94140625e+00 ... -2.70351562e+01\n",
      "    -1.51210938e+01 -4.17070312e+01]\n",
      "   [-2.38370713e+01  1.34293359e+01 -1.34293359e+01 ... -1.08553799e+01\n",
      "    -1.10792021e+01  1.67866699e+00]]\n",
      "\n",
      "  [[-4.23024082e+01 -2.74182275e+01 -2.07035596e+01 ... -7.77782373e+01\n",
      "     1.65516565e+02 -6.21106787e+01]\n",
      "   [-1.86891592e+01  2.40608936e+01 -1.48841807e+01 ...  9.51244629e+00\n",
      "    -1.20864023e+01  3.69306738e+00]\n",
      "   [-1.29033536e+02  1.79057812e+00  4.63312090e+01 ... -1.20752112e+02\n",
      "     6.91610801e+01 -1.34293359e+00]\n",
      "   ...\n",
      "   [ 8.90234375e+00 -1.74609375e+00 -1.51562500e+00 ...  5.68750000e+00\n",
      "    -6.22656250e+00  2.62109375e+00]\n",
      "   [-4.21640625e+01  5.94531250e+00 -2.40234375e+00 ... -2.25546875e+01\n",
      "    -1.55039062e+01 -1.18671875e+01]\n",
      "   [-1.86891592e+01  2.40608936e+01 -1.48841807e+01 ...  9.51244629e+00\n",
      "    -1.20864023e+01  3.69306738e+00]]]]\n",
      "[[[[-1.90248926  0.3357334   2.35013379 ... -0.22382227  0.3357334\n",
      "    -0.11191113]\n",
      "   [-0.22382227 -3.02160059  0.3357334  ... -0.3357334   0.22382227\n",
      "     0.11191113]\n",
      "   [-0.3357334   0.6714668   0.44764453 ... -0.22382227  0.55955566\n",
      "    -0.22382227]\n",
      "   ...\n",
      "   [-0.3125     -0.234375   -0.2734375  ... -0.42578125 -0.42578125\n",
      "    -0.42578125]\n",
      "   [-9.70703125 -9.62890625 -9.625      ... -9.625      -9.58984375\n",
      "    -9.5859375 ]\n",
      "   [-0.22382227 -3.02160059  0.3357334  ... -0.3357334   0.22382227\n",
      "     0.11191113]]\n",
      "\n",
      "  [[-1.45484473  0.78337793  2.12631152 ... -0.44764453  0.55955566\n",
      "    -0.11191113]\n",
      "   [ 0.11191113 -3.35733398  0.         ... -0.22382227  0.22382227\n",
      "    -0.3357334 ]\n",
      "   [ 0.          0.3357334   0.3357334  ... -0.22382227  0.22382227\n",
      "    -0.44764453]\n",
      "   ...\n",
      "   [-0.3125     -0.19921875 -0.2734375  ... -0.46484375 -0.46484375\n",
      "    -0.390625  ]\n",
      "   [-9.62890625 -9.625      -9.625      ... -9.58984375 -9.58984375\n",
      "    -9.625     ]\n",
      "   [ 0.11191113 -3.35733398  0.         ... -0.22382227  0.22382227\n",
      "    -0.3357334 ]]\n",
      "\n",
      "  [[-1.11911133  0.55955566  2.35013379 ... -0.22382227  0.55955566\n",
      "    -0.44764453]\n",
      "   [ 0.44764453 -3.35733398  0.22382227 ... -0.44764453  0.44764453\n",
      "    -0.11191113]\n",
      "   [-0.22382227  0.55955566  0.11191113 ... -0.3357334   0.3357334\n",
      "    -0.22382227]\n",
      "   ...\n",
      "   [-0.3515625  -0.2734375  -0.3515625  ... -0.42578125 -0.46484375\n",
      "    -0.42578125]\n",
      "   [-9.55078125 -9.5859375  -9.62890625 ... -9.625      -9.58984375\n",
      "    -9.6640625 ]\n",
      "   [ 0.44764453 -3.35733398  0.22382227 ... -0.44764453  0.44764453\n",
      "    -0.11191113]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 0.55955566  2.90968945 -1.45484473 ...  0.3357334   0.22382227\n",
      "     0.89528906]\n",
      "   [-1.23102246  0.3357334   0.         ... -0.11191113 -0.22382227\n",
      "     0.55955566]\n",
      "   [ 0.44764453  0.6714668  -0.44764453 ...  0.3357334  -0.22382227\n",
      "    -0.11191113]\n",
      "   ...\n",
      "   [-0.3125     -0.3125     -0.16015625 ... -0.42578125 -0.46484375\n",
      "    -0.46484375]\n",
      "   [-9.62890625 -9.58984375 -9.55078125 ... -9.625      -9.62890625\n",
      "    -9.62890625]\n",
      "   [-1.23102246  0.3357334   0.         ... -0.11191113 -0.22382227\n",
      "     0.55955566]]\n",
      "\n",
      "  [[ 0.89528906  3.13351172 -1.79057812 ...  0.44764453  0.3357334\n",
      "     1.0072002 ]\n",
      "   [-1.79057812  0.55955566  0.         ...  0.3357334   0.44764453\n",
      "     0.55955566]\n",
      "   [ 0.44764453  0.55955566 -0.55955566 ...  0.22382227 -0.22382227\n",
      "    -0.55955566]\n",
      "   ...\n",
      "   [-0.3125     -0.234375   -0.16015625 ... -0.390625   -0.46484375\n",
      "    -0.46484375]\n",
      "   [-9.62890625 -9.6640625  -9.55078125 ... -9.5859375  -9.62890625\n",
      "    -9.58984375]\n",
      "   [-1.79057812  0.55955566  0.         ...  0.3357334   0.44764453\n",
      "     0.55955566]]\n",
      "\n",
      "  [[ 1.0072002   2.79777832 -1.45484473 ...  0.55955566  0.\n",
      "     0.78337793]\n",
      "   [-2.79777832  0.3357334  -0.11191113 ...  0.          0.11191113\n",
      "     0.78337793]\n",
      "   [ 0.3357334   0.6714668  -0.44764453 ...  0.3357334  -0.22382227\n",
      "    -0.11191113]\n",
      "   ...\n",
      "   [-0.234375   -0.2734375  -0.12109375 ... -0.42578125 -0.46484375\n",
      "    -0.50390625]\n",
      "   [-9.66796875 -9.62890625 -9.58984375 ... -9.546875   -9.62890625\n",
      "    -9.546875  ]\n",
      "   [-2.79777832  0.3357334  -0.11191113 ...  0.          0.11191113\n",
      "     0.78337793]]]]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 2 required positional arguments: 'value_index' and 'dtype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-616708894c77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3200\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3300\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3500\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3600\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavez\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'har_ds_label.npz'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 2 required positional arguments: 'value_index' and 'dtype'"
     ]
    }
   ],
   "source": [
    "data[:100].reshape(1,-1).shape\n",
    "np.savez('har_ds_data_.npz', data=data[:100].T, label=)\n",
    "np.savez('har_ds_data_.npz', data=data[856:856+100].T, label=)\n",
    "np.savez('har_ds_data_.npz', data=data[1250:1350].T, label=)\n",
    "np.savez('har_ds_data_.npz', data=data[2550:2650].T, label=)\n",
    "np.savez('har_ds_data_.npz', data=data[3200:3300].T, label=)\n",
    "np.savez('har_ds_data_.npz', data=data[3500:3600].T, label=)\n",
    "np.savez('har_ds_data_.npz', data=net(tf.Tensor(data[0])))\n",
    "np.savez('har_ds_label.npz', data=data[0], label=np.array([0,0,0,0,0,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_label[0])\n",
    "print(test_label[10])\n",
    "\n",
    "np.savez('har_ds_label_3.npz', data=test_data[0],  label=np.array([0,0,0,1,0]))\n",
    "np.savez('har_ds_label_1.npz', data=test_data[10], label=np.array([0,1,0,0,0]))\n",
    "\n",
    "a = np.load('har_ds_label_3.npz')\n",
    "print(a['data'], a['label'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
